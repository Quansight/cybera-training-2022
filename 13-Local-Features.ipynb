{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- Characteristics of good local features\n",
    "- Raw patches as local features\n",
    "- Shift Invariant Feature Transform (SIFT)\n",
    "- Matching local features\n",
    "- Blob detection\n",
    "- MSER in OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Motivation\n",
    "\n",
    "Consider image stitching.  It requires that we find corresponding \"locations\" in two images.  Given these corresponding locations, we can compute *homography*, which would allow us to stitch the two images to construct a panorama.\n",
    "\n",
    "<center>\n",
    "  <tr>\n",
    "      <td><img src=\"images/stitching.png\" width=\"75%\"></img></td>\n",
    "  </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Applications of local invariant features\n",
    "\n",
    "- Wide baseline stereo\n",
    "- Motion tracking\n",
    "- Panoramas\n",
    "- Mobile robot navigation\n",
    "- 3D reconstruction\n",
    "- Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "##### Panoramas\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/autostitch.png\" width=\"50%\"></img></td>\n",
    "    </tr><br/>\n",
    "    (Figure from UBC autostich)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "##### Wide base-line stereo\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/wide-baseline-stereo.png\" width=\"50%\"></img></td>\n",
    "    </tr><br/>\n",
    "    (Image from T. Tuytelaars ECCV 2006 tutorial)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "##### Object recogniton\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/object-recognition.png\" width=\"50%\"></img></td>\n",
    "    </tr><br/>\n",
    "    (Figure from Kristen Grauman)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Review: Characteristics of a Good Feature\n",
    "\n",
    "- Repeatability — feature is invariant to geometric, lighting, etc. changes \n",
    "- Saliency or distinctiveness — how well features stand out from other features\n",
    "- Compactness — efficiency, many fewer features than the number of pixels in the image\n",
    "- Locality — robustness to clutter and occlusion, a feature should only occupy a small area of an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Repeatability\n",
    "\n",
    "We need to find at least some of the same points in two images to any chance of finding true matches.  There is little chance that we can find corresponding locations given the following two images.\n",
    "\n",
    "<center>\n",
    "  <tr>\n",
    "      <td><img src=\"images/local-feature-repeatability1.png\" width=\"75%\"></img></td>\n",
    "  </tr>\n",
    "</center>\n",
    "\n",
    "Detection process run independently on two images should return at least some of the corresponding locations as seen below.\n",
    "\n",
    "<center>\n",
    "  <tr>\n",
    "      <td><img src=\"images/local-feature-repeatability2.png\" width=\"75%\"></img></td>\n",
    "  </tr>\n",
    "</center>\n",
    "\n",
    "Recall that we have attempted to address this issue by interest point detection.  These are locations in the image that are (somewhat) \"invariant\" to geometric and photometric changes.  Specifically, we identified corner locations as those that are covariant to translation and rotation and partially invariant to changes in intensity.  Recall also that corner detection is *not* invariant to changes in scale.\n",
    "\n",
    "**Observation 1**: identify interest points locations (say, through corner detection) and construct local features around these locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "##### Interest point detectors\n",
    "\n",
    "Available interest point detectors\n",
    "\n",
    "- Hessian & Harris [Beaudet 78], [Harris 88]\n",
    "- Laplacian, Difference of Gaussian (DoG) [Lindeberg 98], [Lowe 99]\n",
    "- Harris-/Hessian-Laplace [Mikolajczyk & Schmid 01]\n",
    "- Harris-/Hessian-Affine [Mikolajczyk & Schmid 04]\n",
    "- Edge-based Region Detector (EBR) and (Intensity Extrema Based Region Detector) IBR [Tuytelaars & Van Gool 04] \n",
    "- Maximally Stable Extremal Regions (MSER) [Matas 02]\n",
    "- Salient Regions [Kadir & Brady 01] \n",
    "- and many others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "##### Which interest point detector should you choose?\n",
    "\n",
    "What do you want it for?\n",
    "\n",
    "- Precise localization in x-y: Harris\n",
    "- Good localization in scale: DoG\n",
    "- Flexible region shape: MSER\n",
    "\n",
    "Best choice often application-dependent\n",
    "\n",
    "- Harris-/Hessian-Laplace/DoG work well for many natural categories\n",
    "- MSER works well for buildings and printed things\n",
    "\n",
    "Take-home lesson\n",
    "\n",
    "- There have been extensive evaluations/comparisons [Mikolajczyk et al., IJCV 05, PAMI 05].  Best to check these out and select the best interest point detector for your application.  \n",
    "- It is sometimes useful to use multiple detectors simultaneously to help with matching over a range of image categories\n",
    "\n",
    "<font color=\"purple\">\n",
    "We will soon see that deep learning has revolutionized image feature construction.  More on this later.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Saliency\n",
    "\n",
    "We want to reliably determine which location in one image goes with which location in the second image.  The computed features should be invariant to geometric and photometric differences between the two images.  Consider the following figure\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/local-feature-saliency.png\" width=\"75%\"></img></td>\n",
    "    </tr>\n",
    "</center>\n",
    "\n",
    "Our task is to find the corresponding locations in the two images.  This means that we need to figure out which of the two locations in the image on the right matches with the location shown in the image on the left.\n",
    "\n",
    "**Observation 2**: compute descriptors that encode the area surrounding an interest point.  These descriptors should be compact (for computational reasons), these should have local support, and these should have invariance properties with respect to geometric and photometric changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "##### Local feature descriptors\n",
    "\n",
    "Encode area around interest points as vectors.  We can then easily match these features to identify the corresponding locations between the two images.  The following figure illustrates this idea.  Here, local area around three interest point locations (one in the left image, and two in the right image) is encoded as $d$-dimensional vectors. \n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/local-feature-computation.png\" width=\"100%\"></img></td>\n",
    "    </tr>\n",
    "</center>\n",
    "\n",
    "We can find the corresponding location by matching these $d$-dimensional vectors.  There are many options for doing so.  E.g., we can uses sum-of-squared differences (SSD) to match these vectors.  Alternately, we can use cosine similarity.  And there are many other techniques for matching vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "##### Obstacles to computing local feature descriptors\n",
    "\n",
    "Invariance to translation, rotation, and scale.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/local-features-geometric.png\" width=\"50%\"></img></td>\n",
    "    </tr>\n",
    "</center>\n",
    "\n",
    "Invariance to changes in intensity and color.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/local-features-photometric.png\" width=\"50%\"></img></td>\n",
    "    </tr>\n",
    "</center>\n",
    "\n",
    "(Figures courtesy T. Tuytelaars ECCV 2006 tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Raw patches as local descriptors\n",
    "\n",
    "The simplest way to describe the neighborhood around an interest point is to write down the list of intensities to form a feature vector.  \n",
    "Consider the figure below.  \n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/local-features-construction.png\" width=\"50%\"></img></td>\n",
    "    </tr>\n",
    "</center>\n",
    "\n",
    "The image patch around the interest point locations (depicted by the red circles) are as seen below.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/local-features-construction-2.jpg\" width=\"30%\"></img></td>\n",
    "        <td><img src=\"images/local-features-construction-1.jpg\" width=\"30%\"></img></td>\n",
    "    </tr>\n",
    "</center>\n",
    "\n",
    "Let's write down the list of intensities in these patches to form the feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "left_patch = cv.imread('data/local-features-construction-2.jpg')\n",
    "left_patch = cv.cvtColor(left_patch, cv.COLOR_BGR2RGB)\n",
    "left_patch = cv.resize(left_patch, (32, 32), interpolation=cv.INTER_NEAREST)\n",
    "\n",
    "right_patch = cv.imread('data/local-features-construction-1.jpg')\n",
    "right_patch = cv.cvtColor(right_patch, cv.COLOR_BGR2RGB)\n",
    "right_patch = cv.resize(right_patch, (32, 32), interpolation=cv.INTER_NEAREST)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(left_patch)\n",
    "plt.subplot(122)\n",
    "plt.imshow(right_patch);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Notice that listing intensities is very sensitive to changes in rotation, scale, intensity, etc.  These make poor feature descriptors.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/local-features-construction-3.png\" width=\"50%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Shift Invariant Feature Transform (SIFT) [Lowe 2004]\n",
    "\n",
    "Description taken from various places, including [https://sbme-tutorials.github.io/2019/cv/notes/7_week7.html](https://sbme-tutorials.github.io/2019/cv/notes/7_week7.html)\n",
    "\n",
    "- Finds features invariant to scale and rotation\n",
    "- Partially invariant to illumination changes, camera viewpoint, occlusion, and clutter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### SIFT Pyramid\n",
    "\n",
    "Construct SIFT pyramid, which consists of Octaves and Scales. Octaves are different levels of image resolutions (pyramids levels), and scales\n",
    "represent different scales of window in each octave level (different $\\sigma$ of Gaussian window)\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/sift_dog.jpg\" width=\"80%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Key-point localization in scale\n",
    "\n",
    "At each scale compare cornerness with neighbouring scales (upper and lower scales) and pick the scale with maximum cornerness value.  Not all corners in an image are localized at the same scale.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/sift_local_extrema.jpg\" width=\"40%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Computing SIFT descriiptor\n",
    "\n",
    "- Use image gradients instead of raw intensities\n",
    "- Use histograms to bin pixels (gradients) within sub-patches according to their orientation.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/sift.png\" width=\"80%\"></img></td>\n",
    "    </tr>\n",
    "</center>\n",
    "\n",
    "- Use of image gradients provides partial invariance to changes in illumination\n",
    "- Using subpatches maintains spatial structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Achieving rotation invariance\n",
    "\n",
    "Rotate patch according to its dominant gradient orientation.  This puts the patches into a canonical orientation.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/sift-rotation.png\" width=\"70%\"></img></td>\n",
    "    </tr><br/>\n",
    "    (Image from Matthew Brown.)\n",
    "</center>\n",
    "\n",
    "See below for how to find the dominant gradient orientation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### 128-dimensional SIFT Descriptors\n",
    "\n",
    "After localization of a key-point in our scale space. We can get its SIFT descriptor as follow\n",
    "\n",
    "- Extract a $16 \\times 16$ window centered by this point.\n",
    "- Get gradient magnitude and multiply it by a $16 \\times 16$ Gaussian window of $\\sigma=1.5$\n",
    "- Get gradient angle direction.\n",
    "- Adjusting orientation (To be rotation invariant): get the gradient angle of the window and Quantize them to 36 values $(0, 10, 20, \\cdots, 360)$\n",
    "- Locate dominant corner direction which is most probable angle (angle with max value in 36 bit angle histogram) subtract dominant direction from gradient angle.\n",
    "- For each block get magnitude weighted angle histogram and normalize it (divide by total gradient magnitudes).  Here angles are quantized to 8 angles [0, 45, 90, … , 360] based on its relevant gradient magnitude i.e (histogram of angle 0 = sum(all magnitudes with angle 0)).\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/sift_oriented.png\" width=\"80%\"></img></td>\n",
    "    </tr>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/sift_fingerprint.jpg\" width=\"80%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### SIFT properties\n",
    "\n",
    "- Extraordinarily robust matching technique\n",
    "- Can handle changes in viewpoint of up to about 60 degree out of plane rotation\n",
    "- Can handle significant changes in illumination\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/sift-example.png\" width=\"95%\"></img></td>\n",
    "    </tr><br/>\n",
    "    (Image from Steve Seitz)\n",
    "</center>\n",
    "\n",
    "- Fast and efficient—can run in real time\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/sift-example-2.png\" width=\"95%\"></img></td>\n",
    "    </tr><br/>\n",
    "    NASA Mars Rover images with SIFT feature matches. (Figure by Noah Snavely.)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### SIFT Code\n",
    "\n",
    "SIFT was initially included in OpenCV; however, it is no longer available.  Since SIFT was patented.  An option is to use [VLfeat library](https://www.vlfeat.org/), which includes a SIFT implementation.  VLfeat currently doesn't have a \"stable\" Python biding.  Still you are welcome to try it using `pip install pyvlfeat`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Matching local features\n",
    "\n",
    "Consider the figures below.  \"SIFT patches\" are overlaid on the two images.  Our goal is to generate candidate matches.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/local-feature-matching-2.png\" width=\"90%\"></img></td>\n",
    "    </tr>\n",
    "</center>\n",
    "\n",
    "Classical feature descriptors, such as SIFT, SURF, etc., are usually compared and matched using the Euclidean distance (or L2-norm).  Other techniques for matching these features are Cosine similarity, Earth Mover's Distance (also known as *Wasserstein Distance*), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Distance computation in Python\n",
    "\n",
    "Check out `scipy.spatial.distance` module for various methods for computing distance matrix for a collection of raw observation vectors stored in a rectangular array.\n",
    "\n",
    "See [here](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html#module-scipy.spatial.distance) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Example of Euclidean distance\n",
    "\n",
    "Compute the Euclidean distance matrix between the four vectors $[1,0,0]$, $[0,1,0]$, $[1,1,0]$, and $[10,-2,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([1,0,0])\n",
    "b = np.array([0,1,0])\n",
    "c = np.array([1,1,0])\n",
    "d = np.array([10,-2,1])\n",
    "\n",
    "# Set up an m-by-n matrix, where m is the number of\n",
    "# data items and n is the dimension\n",
    "X = np.vstack((a,b,c,d))\n",
    "#print(f'Shape of X is {X.shape}')\n",
    "\n",
    "# D is condensed matrix\n",
    "metric = 'euclidean'\n",
    "i = 1\n",
    "D = distance.pdist(X, metric)\n",
    "\n",
    "# Lets convert it into square form\n",
    "print(f'{metric} distances:')\n",
    "labels = list('abcd')\n",
    "display(DataFrame(data=distance.squareform(D), index=labels, columns=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### From distance to similarity\n",
    "\n",
    "How do we convert distance values to similarity values?  For cosine distance, simply subtract cosine distance from 1.0.  In general if your distance metric returns values between 0 and 1, then you can use this trick.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Example of cosine similarity\n",
    "\n",
    "Compute Cosine similarity matrix between $[1,0,0]$, $[0,1,0]$, $[1,1,0]$, and $[10,-2,1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.array([1,0,0])\n",
    "b = np.array([0,1,0])\n",
    "c = np.array([1,1,0])\n",
    "d = np.array([10,-2,1])\n",
    "\n",
    "# Set up an m-by-n matrix, where m is the number of\n",
    "# data items and n is the dimension\n",
    "X = np.vstack((a,b,c,d))\n",
    "#print(f'Shape of X is {X.shape}')\n",
    "\n",
    "# D is condensed matrix\n",
    "metric = 'cosine'\n",
    "i = 0\n",
    "D = distance.pdist(X, metric)\n",
    "\n",
    "# Lets convert it into square form\n",
    "print(f'{metric} similarity:')\n",
    "print(1.0 - distance.squareform(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "For other distances, we can use, say, a *Gaussian kernel* as follows:\n",
    "\n",
    "$$\n",
    "K(d) = \\exp \\left( \\frac{d^2} {2 \\sigma^2}  \\right), \n",
    "$$\n",
    "\n",
    "where $d$ is the distance between two vectors $\\mathbf{x}_1$ and $\\mathbf{x}_2$.  $\\sigma$ is a tuning (or scaling) parameter.  If $\\sigma$ is high, $K(d)$ will be close to $1$ (i.e., high similarity) for large values of $d$.  If $\\sigma$ is small, even a small $d$ will reduce the similarity scores for the two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Example: Gaussian kernel for Euclidean distance to similarity\n",
    "\n",
    "Compute similarity matrix between $[1,0,0]$, $[0,1,0]$, $[1,1,0]$, and $[10,-2,1]$.  Assume Euclidean distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.array([1,0,0])\n",
    "b = np.array([0,1,0])\n",
    "c = np.array([1,1,0])\n",
    "d = np.array([10,-2,1])\n",
    "\n",
    "# Set up an m-by-n matrix, where m is the number of\n",
    "# data items and n is the dimension\n",
    "X = np.vstack((a,b,c,d))\n",
    "#print(f'Shape of X is {X.shape}')\n",
    "\n",
    "# D is condensed matrix\n",
    "metric = 'euclidean'\n",
    "labels = list('abcd')\n",
    "D = distance.squareform(distance.pdist(X, metric))\n",
    "print('Distance:')\n",
    "display(DataFrame(data=D, index=labels, columns=labels))\n",
    "\n",
    "# Lets convert it into square form\n",
    "sigma = 4\n",
    "scaling = 2 * (sigma ** 2)\n",
    "\n",
    "print(f'{metric} similarity:')\n",
    "#np.set_printoptions(formatter={'float': lambda x: \"{0:0.1e}\".format(x)})\n",
    "S = np.exp(-D**2 / (scaling))\n",
    "display(DataFrame(data=S, index=labels, columns=labels))\n",
    "#np.set_printoptions() # To not mess with other printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Wasserstein distance\n",
    "\n",
    "Wasserstien distance is computed between two probability distributions (below represented as histograms).  Check out `scipy.stats` module for methods for computing Wasserstien distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import wasserstein_distance\n",
    "wasserstein_distance([0, 1, 3], [5, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Hamming distance\n",
    "\n",
    "We now also have binary feature descritors, such as ORB, BRISK, which are matched using Hamming distance.  \n",
    "\n",
    "$$\n",
    "d_{\\mathrm{hamming}} (\\mathbf{a}, \\mathbf{b}) = \\sum_{i=0}^{n-1} (a_i \\oplus b_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "*Aside*:\n",
    "\n",
    "- SIFT descriptors represent the histogram of oriented gradient in a neighbourhood\n",
    "- SURF descriptors represent the histogram of of the Haar wavelet response in a neighborhood\n",
    "- [See here](https://medium.com/data-breach/introduction-to-brief-binary-robust-independent-elementary-features-436f4a31a0e6) for more information about Binary Robust Independent Elementary Features (BRIEF)\n",
    "- Oriented Fast and Rotated Brief (ORB) [Ethan Rublee et al. 2011]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Brute force matching\n",
    "\n",
    "Compare distances with, take the closest (or closest $k$, or within a thresholded distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img1 = cv.imread('data/box.png',cv.IMREAD_GRAYSCALE)          \n",
    "img2 = cv.imread('data/box_in_scene.png',cv.IMREAD_GRAYSCALE) \n",
    "print(img1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orb = cv.ORB_create()\n",
    "kp1, des1 = orb.detectAndCompute(img1, None) # locations and descriptor\n",
    "kp2, des2 = orb.detectAndCompute(img2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1, des2)\n",
    "matches = sorted(matches, key = lambda x:x.distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img3 = cv.drawMatches(img1, kp1, img2, kp2, matches[:10], None, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### KDTree data structure\n",
    "\n",
    "See [here](https://en.wikibooks.org/wiki/Data_Structures) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "X = rng.random_sample((10, 3))\n",
    "print(X)\n",
    "T = KDTree(X, leafsize=3)\n",
    "distance, index = T.query(X[0,:]) # Try to perturb the query vector +[0.01,0.01,0]\n",
    "print(f'distance={distance}, data={X[index,:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Ambiguous matches\n",
    "\n",
    "Lets consider SSD metric for finding matches.  How do we threshold on SSD?  One approach is to compute the ratio of the distance to best match to distance to the second best match.   If this ratio is low, the best match is a good candidate.  If this ratio is high, then the best match could be an ambiguous match.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/local-feature-matching-3.png\" width=\"90%\"></img></td>\n",
    "    </tr><br/>\n",
    "    (Figure from Lowe 2004)\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### FLANN matching\n",
    "\n",
    "Check [this](https://www.cs.ubc.ca/research/flann/uploads/FLANN/flann_pami2014.pdf) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "(From OpenCV documentation) FLANN stands for Fast Library for Approximate Nearest Neighbors. It contains a collection of algorithms, such as KDTree, [Locality Sensitive Hashing](https://www.mit.edu/~andoni/LSH/), etc., optimized for fast nearest neighbor search in large datasets and for high dimensional features. It works more faster than BFMatcher for large datasets.\n",
    "\n",
    "For OpenCV implementation, possible values are:\n",
    "\n",
    "- `FLANN_INDEX_LINEAR = 0`\n",
    "- `FLANN_INDEX_KDTREE = 1`\n",
    "- `FLANN_INDEX_KMEANS = 2`\n",
    "- `FLANN_INDEX_COMPOSITE = 3`\n",
    "- `FLANN_INDEX_KDTREE_SINGLE = 4`\n",
    "- `FLANN_INDEX_HIERARCHICAL = 5`\n",
    "- `FLANN_INDEX_LSH = 6`\n",
    "- `FLANN_INDEX_SAVED = 254`\n",
    "- `FLANN_INDEX_AUTOTUNED = 255`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img1 = cv.imread('data/box.png',cv.IMREAD_GRAYSCALE)          \n",
    "img2 = cv.imread('data/box_in_scene.png',cv.IMREAD_GRAYSCALE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orb = cv.ORB_create()\n",
    "kp1, des1 = orb.detectAndCompute(img1, None) # locations and descriptor\n",
    "kp2, des2 = orb.detectAndCompute(img2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FLANN_INDEX_LSH = 6\n",
    "index_params = dict(algorithm = FLANN_INDEX_LSH, table_number = 6)\n",
    "search_params = dict(checks=50)\n",
    "flann = cv.FlannBasedMatcher(index_params, search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matches = flann.knnMatch(des1, des2, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "good = []\n",
    "for match in matches:\n",
    "    if len(match) < 2: continue\n",
    "    m, n = match[0], match[1]\n",
    "    if m.distance < 0.75*n.distance:\n",
    "        good.append([m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img3 = cv.drawMatchesKnn(img1, kp1, img2, kp2, good, None, flags=cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Blob detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Recall ideas underlying edge detection.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/edge-detection-blob.png\" width=\"95%\"></img></td>\n",
    "    </tr><br/>\n",
    "    (Figure from Steve Seitz).\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Second derivative of Gaussian (Laplacian)\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/laplacian-blob.png\" width=\"95%\"></img></td>\n",
    "    </tr><br/>\n",
    "    (Figure from Steve Seitz).\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### From edges to blobs\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/blob-detection.png\" width=\"95%\"></img></td>\n",
    "    </tr><br/>\n",
    "    (Figure from Lana Lazebnik).\n",
    "</center>\n",
    "\n",
    "- Edge = ripple\n",
    "- Blob = superposition of two ripples\n",
    "- Spatial selection: the magnitude of the Laplacian response will achieve a maximum at the center of the blob, provided the scale of the Laplacian is \"matched\" to the scale of the blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Blob detection in 2D\n",
    "\n",
    "Laplacian of Gaussian is circularly symmetric operator for blob detection in 2D.\n",
    "\n",
    "$$\n",
    "\\nabla^2 g = \\frac{\\partial^2 g}{\\partial x^2} + \\frac{\\partial^2 g}{\\partial y^2}\n",
    "$$\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/laplacian.png\" width=\"55%\"></img></td>\n",
    "    </tr><br/>\n",
    "    (Figure from Lana Lazebnik).\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Characteristic scale\n",
    "\n",
    "We define the characteristic scale as the scale that produces peak of Laplacian response.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/characteristic-scale.png\" width=\"95%\"></img></td>\n",
    "    </tr><br/>\n",
    "    (Figure from Lana Lazebnik).\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"blob-detection-slides/feature-detection.001.png\" width=\"90%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"blob-detection-slides/feature-detection.002.png\" width=\"90%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"blob-detection-slides/feature-detection.003.png\" width=\"90%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"blob-detection-slides/feature-detection.004.png\" width=\"90%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"blob-detection-slides/feature-detection.005.png\" width=\"90%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"blob-detection-slides/feature-detection.006.png\" width=\"90%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"blob-detection-slides/feature-detection.007.png\" width=\"90%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#### Difference of Gaussians\n",
    "\n",
    "We can approximate Laplacian as Difference of Gaussian (DOG), which much more efficient to compute.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/laplacian-dog.png\" width=\"50%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blob detection example in OpenCV\n",
    "\n",
    "The relevant parameters are described below:\n",
    "\n",
    "- Area: filter the blobs based on size\n",
    "- Circularity: a measure of how close the blob is to a circle.  Circularity is defined by $\\frac{4 \\pi\\ \\mathrm{area}}{\\mathrm{parameter}}$.\n",
    "- Convexity: the ratio of area of the blob and the are of its convex hull.  Convexity values lie between $0$ and $1$, inclusive.\n",
    "- Inertia: the measure of \"ellipseness\" of a shape.  A circle has inertia of $1$ and a line has inertia of $0$.  The inertia of an ellipse lies somewhere between $0$ and $1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "filename = \"data/butterfly.jpg\"\n",
    "#filename = \"data/BlobTest.jpg\"\n",
    "im = cv.imread(filename)\n",
    "im = cv.cvtColor(im, cv.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "params = cv.SimpleBlobDetector_Params()\n",
    "params.minThreshold = 10 # Change thresholds\n",
    "params.maxThreshold = 250\n",
    "params.filterByArea = False # Filter by Area.\n",
    "params.minArea = 100\n",
    "params.filterByCircularity = False # Filter by Circularity\n",
    "params.minCircularity = 0.1\n",
    "params.filterByConvexity = False # Filter by Convexity\n",
    "params.minConvexity = 0.9\n",
    "params.filterByInertia = False # Filter by Inertia\n",
    "params.minInertiaRatio = 0.9\n",
    "detector = cv.SimpleBlobDetector_create(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "keypoints = detector.detect(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "im_with_keypoints = cv.drawKeypoints(im, keypoints, np.array([]), (255,0,0), cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(im_with_keypoints);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Maximally Stable Extremal Region (MSER)\n",
    "\n",
    "MSER detects homogeneous regions.  We can use the centroids of these regions as keypoint locations.\n",
    "\n",
    "MSER are affine invariant, which means that image skew or warping doesn't effect these.  In addition, MSER are also \"partially\" invariant to changes in image intensity.  In addition, MSER have the followign useful properties:\n",
    "\n",
    "- Multi-scale detection without any smoothing involved, both fine and large structure is detected\n",
    "- Only regions whose support is nearly the same over a range of thresholds is selected, which leads to stability\n",
    "- The set of all extremal regions can be enumerated in worst-case $\\mathcal{O} (n)$, where $n$ is the number of pixels in the image.\n",
    "- Covariance to adjacency preserving (continuous) transformation $T: D \\to D$\n",
    "\n",
    "For more information, check [this](https://en.wikipedia.org/wiki/Maximally_stable_extremal_regions) wiki article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### MSER in OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "im = cv.imread('data/box.png');\n",
    "im = cv.cvtColor(im, cv.COLOR_BGR2RGB)\n",
    "vis = im.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mser = cv.MSER_create()\n",
    "regions, bboxes = mser.detectRegions(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "hulls = [cv.convexHull(p.reshape(-1, 1, 2)) for p in regions]\n",
    "cv.polylines(vis, hulls, 1, (255, 0, 255), 1)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(vis);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Based on materials from Prof. Faisal Qureshi (Faculty of Science, Ontario Tech University, Oshawa ON, Canada, http://vclab.science.ontariotechu.ca)\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
