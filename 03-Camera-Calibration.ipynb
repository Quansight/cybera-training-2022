{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34481285-463f-4db9-a837-144470f9dfe2",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fad7e-b0db-424d-996a-d664b3604b05",
   "metadata": {},
   "source": [
    "# Camera Calibration\n",
    "\n",
    "We have established that projective matrix \n",
    "\n",
    "$$\n",
    "\\mathtt{P} = \n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "p_{11} & p_{12} & p_{13} & p_{14} \\\\ \n",
    "p_{21} & p_{22} & p_{23} & p_{24} \\\\ \n",
    "p_{31} & p_{32} & p_{33} & p_{34} \n",
    "\\end{array}\n",
    "\\right]\n",
    "$$ \n",
    "\n",
    "encodes useful properties about camera.  This matrix controls the imaging process:\n",
    "$\n",
    "\\mathbf{x} = \\mathtt{P} \\mathbf{X}.\n",
    "$\n",
    "\n",
    "We can also use this matrix to back project a point in the image, which can be used to perform metric scene analysis.  The question remains, however.  Given a camera, how do we actually find this matrix?  The process of finding $\\mathtt{P}$ or its variant components is referred to *Camera Calibration*.\n",
    "\n",
    "A detailed treatment of this subject is left as a self-study exercise.  Here we present the basic idea.  Say we have a number of point correspondences $\\mathbf{X}_i \\longleftrightarrow \\mathbf{x}_i$ between 3D points $\\mathbf{X}_i$ and 2D image points $\\mathbf{x}_i$.  We are interested in finding the camera matrix $\\mathtt{P}$ such that for all $i$\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i = \\mathtt{P} \\mathbf{X}_i.\n",
    "$$\n",
    "\n",
    "With some patience, we can re-arrange the above as follows\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "\\mathbf{0}^T & -w_i \\mathbf{X}_i^T & y_i \\mathbf{X}_i^T \\\\\n",
    "w_i \\mathbf{X}_i^T & \\mathbf{0}^T & - x_i \\mathbf{X}_i^T \\\\\n",
    "- y_i \\mathbf{X}_i^T & x_i \\mathbf{X}_i^T & \\mathbf{0}^T\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\mathbf{p}^1 \\\\\n",
    "\\mathbf{p}^2 \\\\\n",
    "\\mathbf{p}^3 \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "=\n",
    "\\mathbf{0}.\n",
    "$$\n",
    "\n",
    "Here each $\\mathbf{p}^{iT}$ is a 4-vector, the $i$-th row of $\\mathtt{P}$.  The three equations are linearly dependent, so we often just two the first two.  Specifically, we use\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "\\mathbf{0}^T & -w_i \\mathbf{X}_i^T & y_i \\mathbf{X}_i^T \\\\\n",
    "w_i \\mathbf{X}_i^T & \\mathbf{0}^T & - x_i \\mathbf{X}_i^T\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\mathbf{p}^1 \\\\\n",
    "\\mathbf{p}^2 \\\\\n",
    "\\mathbf{p}^3 \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "=\n",
    "\\mathbf{0}.\n",
    "$$\n",
    "\n",
    "From a set of $n$ point correspondences, we obtain a $2n \\times 12$ matrix.  Recall that $\\mathtt{P}$ has 11 degrees of freedom, so we just need 11 equations (5 and a half points; we only need either $x$ or $y$ for the 6-th correspondence).\n",
    "\n",
    "### Over-determined solution\n",
    "\n",
    "We often use more than 5 and half points to compute $\\mathtt{P}$.  In this case the goal is to minimze $\\| \\mathtt{A} \\mathbf{p} \\|$ subject to a suitable constraint.  A possible choice is for this constraint is $\\| \\mathbf{p} \\| = 1$.\n",
    "\n",
    "*Aside*:\n",
    "\n",
    "We will return to camera calibration at the end of this discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2100c93-161f-4fc6-bcf5-4e8d929baf6c",
   "metadata": {},
   "source": [
    "## Orthographic cameras\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right: auto; text-align: center; display: block; max-width: 700px;\">\n",
    "<img src=\"images/orthographic-projection.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</figure>\n",
    "\n",
    "- A special case of perspective projection\n",
    "- Center of projection (optical center) is at an *infinite* distance away from the image plane\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "X \\\\\n",
    "Y \\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right)\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "X \\\\\n",
    "Y \\\\\n",
    "Z \\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face6396-849d-45a1-8c7b-d463ffe80c79",
   "metadata": {},
   "source": [
    "## Cameras exhibiting weak perspective\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right: auto; text-align: center; display: block; max-width: 700px;\">\n",
    "<img src=\"images/weak-perspective2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "    <br/><figcaption>Picture from Multiple View Geometry in Computer Vision, Second Edition.</figcaption>\n",
    "</figure>\n",
    "\n",
    "- Object dimensions are much smaller compared to its distance from the camera, e.g., satellite imagery\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "x \\\\\n",
    "y \\\\\n",
    "\\lambda\n",
    "\\end{array}\n",
    "\\right)\n",
    "=\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & \\lambda \n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "X \\\\\n",
    "Y \\\\\n",
    "Z \\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e2f47-b153-41db-91b7-3590d0ee8d1f",
   "metadata": {},
   "source": [
    "## Pushbroom cameras\n",
    "\n",
    "Pushbroom cameras is an abstraction for the type of sensors often used in satellite imaging (SPOTS).  Here camera is a linear sensor array that captures one row of image at each time.  The full image is constructed by moving the camera.  When sensor moves, the sensor plane sweeps out a region of space.\n",
    "\n",
    "Let $\\mathbf{X}$ be a point in space, and lets assume that camera matrix is $\\mathtt{P}$.  Suppose that $\\mathtt{P} \\mathbf{X} = (x, y, w)^T$.  Then the corresponding inhomogeneous image point is $(x, y/w)$.  This assumes that the camera motion is along $x$ direction of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4707e5c3-7bb2-4c6f-afda-eabadd5f634f",
   "metadata": {},
   "source": [
    "## Projective Geometry (Lessons)\n",
    "\n",
    "1. Distant objects appear smaller.\n",
    "2. The same object when imaged using different focal lengths appear to have different sizes.\n",
    "3. Parallel lines meet.\n",
    "4. Angles are not maintained, i.e., perpendicular lines appear meet at angles other than 90 degrees.\n",
    "\n",
    "### Distant Objects Appear Smaller\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right: auto; text-align: center; display: block; max-width: 700px;\">\n",
    "<img src=\"images/pinhole6.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</figure>\n",
    "\n",
    "### Focal Lengths\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right: auto; text-align: center; display: block; max-width: 700px;\">\n",
    "<img src=\"images/pinhole5.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</figure>\n",
    "\n",
    "### Angles are Not Preserved\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right: auto; text-align: center; display: block; max-width: 700px;\">\n",
    "<img src=\"images/pinhole8.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</figure>\n",
    "\n",
    "### Parallel Lines Meet\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right: auto; text-align: center; display: block; max-width: 700px;\">\n",
    "<img src=\"images/pinhole7.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</figure>\n",
    "\n",
    "### Vanishing Points\n",
    "\n",
    "- Each set of parallel lines meet at a different point on the image plane.  This point is called the *vanishing point* for this direction.\n",
    " \n",
    "- Set of parallel lines in the same plane lead to *collinear vanishing points.  The lines formed by these vanishing points is called that horizon for that plane.\n",
    "\n",
    "- Vanishing points can be used for 3D scene analysis\n",
    "\n",
    "- These can be used to spot fake images, e.g., scale or perspective doesn't work\n",
    "\n",
    "Is this a perspective image of four identical buildings?  If not, why not?\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right: auto; text-align: center; display: block; max-width: 700px;\">\n",
    "<img src=\"images/vanishing-points.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "    <br/><figcaption>Figure credit: \"Computer Vision: A Modern Approach\" 2e by D.A. Forsyth</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb7e28-ed6f-4309-8159-1c3d0ac0e8ed",
   "metadata": {},
   "source": [
    "## Lens Effects\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right: auto; text-align: center; display: block; max-width: 700px;\">\n",
    "<img src=\"images/lenses.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</figure>\n",
    "\n",
    "Images captured by pinhole cameras are generally two dark.  In order to allow more light to pass through and reach the sensor plane, we need to increase the size of the pinhole.  This has the effect of making the image blurry.  We can undo this by using a lens that will focus the light onto the image plane.  Lenses, however, introduce radial and chromatic distortions.  \n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right: auto; text-align: center; display: block; max-width: 700px;\">\n",
    "<img src=\"images/lens-effects.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "    <br/><figcaption>Figure credit http://www.solidrop.net</figcaption>\n",
    "</figure>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb86ee0-d348-497a-bf7d-37afcec5b347",
   "metadata": {},
   "source": [
    "## Modeling radial distortion to capture (and undo) lens effects\n",
    "\n",
    "This discussion follows Ch. 7 of Multiple View Geometry in Computer Vision, Second Edition.\n",
    "\n",
    "- $(\\tilde{x}, \\tilde{y})$ is the ideal image position (which obeys linear projection)\n",
    "- $(x_d, y_d)$ is the actual image position (after radial distortion)\n",
    "- $\\tilde{r}$ is the radial distance $\\sqrt{\\tilde{x}^2 + \\tilde{y}^2}$ from the center\n",
    "- $L(\\tilde{r})$ is the distortion factor (we make a simplifying assumption that the distorion is a function of radius only.\n",
    "\n",
    "Then we can write\n",
    "\n",
    "$$\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "x_d \\\\\n",
    "y_d \n",
    "\\end{array}\n",
    "\\right)\n",
    "=\n",
    "L(\\tilde{r})\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\tilde{x} \\\\\n",
    "\\tilde{y} \n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "## Correction of distortion\n",
    "\n",
    "In pixel coordinates, the correction is written as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{x} & = x_c + L(r) (x - x_c) \\\\\n",
    "\\hat{y} & = y_c + L(r) (y - y_c)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here $(x, y)$ are the measured coordinates, $(\\hat{x},\\hat{y})$ are the corrected coordinates,\n",
    "$(x_c, y_c)$ is the center of radial distorion, and $r = \\sqrt{(x-x_c)^2 + (y-y_c)^2}$.\n",
    "\n",
    "We often use Taylor expansion $L(r) = 1 + \\kappa_1 r + \\kappa_2 r^2 + \\cdots$.  The coefficient of radial distortions $\\{\\kappa_1, \\kappa_2, x_c, y_c \\}$ are considered part of the intrinsic matrix $\\mathtt{K}$.  These paramters are often estimated during camera calibration process.  $L(r)$ can be estimated by minimizing the deviation from linear mapping, e.g., the distances of end-points of a line to its mid-point.\n",
    "\n",
    "For further information, we refer the reader to Ch. 7 of [Multiple View Geometry in Computer Vision (2nd Ed.)](https://www.robots.ox.ac.uk/~vgg/hzbook/) by Harley and Zisserman."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b601ae-ff99-4764-ae2d-802c3f8326ec",
   "metadata": {},
   "source": [
    "## Camera calibration in OpenCV\n",
    "\n",
    "Check out the camera-calibration notbook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a3607c-bdc3-4d93-aea1-ad03b806db1a",
   "metadata": {},
   "source": [
    "---\n",
    "Based on materials from Prof. Faisal Qureshi (Faculty of Science, Ontario Tech University, Oshawa ON, Canada, http://vclab.science.ontariotechu.ca)\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "filesystem-cybera",
   "language": "python",
   "name": "conda-env-filesystem-cybera-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
