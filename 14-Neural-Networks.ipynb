{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- Definition of neural networks\n",
    "- Essential components: activation functions, layers, forward & backward propagation\n",
    "- Building a neural network in NumPy\n",
    "  - Hand-coding gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "## Definition of Neural Network\n",
    "\n",
    "*Neural network*: a *function* defined by following components:\n",
    "\n",
    "+ Resulting function: compute $\\hat{y}=f(x)$ by $L$ successive function compositions:\n",
    "\n",
    "  $$\\begin{aligned}\n",
    "  a^{0} &= x &&\\text{(input feature vector)}\\\\\n",
    "  z^{\\ell} &= W^{\\ell}a^{\\ell-1}+b^{\\ell} \\\\\n",
    "  a^{\\ell}  &= g_{\\ell}\\left( z^{\\ell} \\right) &&(\\ell=1,\\dotsc,L) \\\\\n",
    "  \\hat{y} &=a^{L} && \\text{(vector from output layer)}\n",
    "  \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "+ Notation here from Michael Neilsen's [*Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com)\n",
    "+ $W^\\ell$ and $b^\\ell$ are *weight matrices* & *bias vectors* respectively associated with layer $\\ell$ of the network\n",
    "+ Entry $w^{\\ell}_{jk}$ of matrix $W^\\ell$ is the weight parameter associated with the link connecting the $k$th neuron in layer $\\ell-1$ to the $j$th neuron in layer $\\ell$\n",
    "\n",
    "<center>\n",
    "    <a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\"><img src=\"images/tikz16.png\" width=\"750px\"></img></a>\n",
    "</center>\n",
    "\n",
    "+ Vectors $z^{\\ell}\\in\\mathbb{R}^{n_{\\ell}\\times 1}$ are *weighted inputs*\n",
    "+ Vectors $a^{\\ell}\\in\\mathbb{R}^{n_{\\ell}\\times 1}$ are *activations*\n",
    "+ Functions $g_{\\ell}$ are *activation functions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "#### Example Neural Network\n",
    "<center>\n",
    "    <img src=\"./images/NN-00.png\" width=\"750px\"></img>\n",
    "</center>\n",
    "\n",
    "+ Example has 2 hidden layers & an output layer, so $L=3$ layers\n",
    "+ Number of units in each layer are $$n_{0}=5, n_{1}=3, n_{2}=4, n_{3}=2$$\n",
    "+ Each layer has an activation functions: $g_{1}$, $g_{2}$, $g_{3}$\n",
    "  + Hidden layer 1 has weight matrix $W^{1}\\in\\mathbb{R}^{3\\times 5}$, bias vector $b^{1}\\in\\mathbb{R}^{3\\times1}$\n",
    "  + Hidden layer 2 has weight matrix $W^{2}\\in\\mathbb{R}^{4\\times 3}$, bias vector $b^{2}\\in\\mathbb{R}^{4\\times1}$\n",
    "  + Output layer has weight matrix $W^{3}\\in\\mathbb{R}^{2\\times 4}$, bias vector $b^{3}\\in\\mathbb{R}^{2\\times1}$\n",
    "   $$\\Rightarrow (5+1)\\times3 + (3+1)\\times4 + (4+1)\\times2 = 44\\ \\text{parameters}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "## How to set up/use Neural Network\n",
    "\n",
    "1. Choose *architecture* of neural network:\n",
    "   + $L$ (# hidden layers + output layer)\n",
    "   + $n_{0}, \\dotsc, n_{L}$ (# units input + in layers)\n",
    "   + $g_{1}, \\dotsc, g_{L}$ (activation functions for each layer)\n",
    "2. Choose appropriate *objective* or *loss function*\n",
    "3. Solve corresponding optimization problem (\"fitting to training data\")\n",
    "\n",
    "+ Hidden layers & nonlinear activation functions encode *feature engineering*\n",
    "+ A lot of experimentation typically required to get appropriate architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Activation functions\n",
    "<center>\n",
    "    <img src=\"./images/signum.png\"></img>\n",
    "</center>\n",
    "\n",
    "+ $\\text{sign}$ (sometimes called \"signum\" or \"step\" function) defined as\n",
    "$$ \\displaystyle{\\text{sign}(t) := \\begin{cases} +1, & \\text{if $t\\ge0$}\\\\ -1, & \\text{if $t<0$} \\end{cases}} $$\n",
    "+ used in *perceptron* (earliest classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "    <img src=\"./images/sigmoid.png\"></img>\n",
    "</center>\n",
    "\n",
    "+ $\\sigma$ (\"sigmoid\" or \"logistic\" function) defined as\n",
    "   $$ \\displaystyle{\\sigma(t) := \\frac{e^{t}}{1 + e^{t}}} $$\n",
    "+ commonly used for output layer activation function *logistic regression* (e.g, *binary classification* models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "    <img src=\"./images/id.png\"></img>\n",
    "</center>\n",
    "\n",
    "+ $\\text{id}$ (identity function) defined as\n",
    "$$ \\displaystyle{\\text{id}(t) := t} $$\n",
    "+ implicitly used as output layer activation function in *regression* models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "+ $\\text{softmax}$: maps vectors to vectors of same length \n",
    "+ Given $t\\in\\mathbb{R}^{K}$, $\\text{softmax}(t)$ defined by\n",
    "  $$ \\displaystyle{[\\text{softmax}(t)]_{k} := \\frac{e^{t_{k}}}{\\sum_{i=1}^{K} e^{t_{i}}}} $$\n",
    "+ $\\text{softmax}(t)$ has nonnegative values summing to one (probabilities)\n",
    "+ Used for *multi-class classification* problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "+ For *regression* problems, typically *mean-squared error* as loss:\n",
    "  $$ \\mathcal{L}(\\hat{y}, y) = \\frac{1}{N} \\sum_{k=1}^{N}  \\left[ y_{k} - \\hat{y}_{k} \\right]^2 $$\n",
    "\n",
    "+ For *classification* problems, typically *cross-entropy* (or *log loss*) as loss:\n",
    "  $$ \\mathcal{L}(\\hat{y}, y) = -\\sum_{k=1}^{N} \\left[ y_{k} \\log\\left( \\hat{y}_{k} \\right) + \\left(1-y_{k}\\right) \\log\\left(1- \\hat{y}_{k} \\right) \\right]$$\n",
    "  (extension exists for multi-class classification problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "+ *Convolutional* neural networks use *convolution matrices* (e.g., image/signal processing)\n",
    "+ *Recurrent* neural networks permit *loops* (e.g., text processing)\n",
    "+ *Backpropagation*: strategy for computing gradients for iterative optimizers\n",
    "+ *Deep learning*: loosely, neural network with several hidden layers\n",
    "+ Vast number of frameworks (e.g., TensorFlow, Theano, PyTorch, etc.); start with those first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Building a neural network in NumPy\n",
    "\n",
    "+ use same activation function in all layers: the *logistic* or *sigmoid* function\n",
    "+ use a functional programming style to help build intuition\n",
    "  + introduce dictionary `model` to store all data associated with the neural network (weight matrices, bias vectors, etc.)\n",
    "+ production codes typically use object-oriented style\n",
    "+ production codes optimized for efficiency (unlike what we'll develop here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise: Implement activation function(s) & their derivatives\n",
    "\n",
    "For today's purposes, we'll use the same activation function in every layer of the network, namely the *logistic* or *sigmoid* function:\n",
    "$$ \\sigma(x) = \\frac{1}{1+\\exp(-x)} = \\frac{\\exp(x)}{1+\\exp(x)}.$$\n",
    "A bit of calculus shows that\n",
    "$$ \\sigma'(x) = \\sigma(x)(1-\\sigma(x)) .$$\n",
    "\n",
    "Actually, a more numerically robust formula for $\\sigma(x)$ (i.e., one that works for large positive or large negative input equally well) is\n",
    "$$\n",
    "\\sigma(x) = \\begin{cases} \\frac{1}{1+\\exp(-x)} & (x\\ge0) \\\\ 1 - \\frac{1}{1+\\exp(x)} & \\mathrm{otherwise} \\end{cases}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    '''The logistic function; accepts arbitrary arrays as input (vectorized)'''\n",
    "    # your code here...\n",
    "def sigma_prime(x):\n",
    "    '''The *derivative* of the logistic function; accepts arbitrary arrays as input (vectorized)'''\n",
    "    # your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Solution:**\n",
    "```python\n",
    "def sigma(x):\n",
    "    '''The logistic function; accepts arbitrary arrays as input (vectorized)'''\n",
    "    return np.where(x>=0, 1/(1+np.exp(-x)), 1 - 1/(1+np.exp(x))) # piecewise-defined for numerical robustness\n",
    "def sigma_prime(x):\n",
    "    '''The *derivative* of the logistic function; accepts arbitrary arrays as input (vectorized)'''\n",
    "    return sigma(x)*(1-sigma(x)) # Derivative of logistic function\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise: Implement loss function & its gradient\n",
    "\n",
    "For the loss function, we'll use the typical \"$L_2$-norm of the error\" (alternatively called *mean-square error (MSE)* when averaged over a batch of values):\n",
    "$$ \\mathcal{L}(\\hat{y},y) = \\frac{1}{2} \\|\\hat{y}-y\\|^{2} = \\frac{1}{2} \\sum_{k=1}^{d} \\left[ \\hat{y}_{k}-y_{k} \\right]^{2}.$$\n",
    "Again, using multivariable calculus, we can see that\n",
    "$$\\nabla_{\\hat{y}} \\mathcal{L}(\\hat{y},y) = \\hat{y} - y.$$\n",
    "\n",
    "Implement both of these functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(yhat, y):\n",
    "    '''The loss as measured by the L2-norm squared of the error'''\n",
    "    # your code here...\n",
    "def loss_prime(yhat, y):\n",
    "    '''Implementation of the gradient of the loss function'''\n",
    "    # your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Solution:**\n",
    "```python\n",
    "def loss(yhat, y):\n",
    "    '''The loss as measured by the L2-norm squared of the error'''\n",
    "    return 0.5 * np.square(yhat-y).sum()\n",
    "def loss_prime(yhat, y):\n",
    "    '''Implementation of the gradient of the loss function'''\n",
    "    return (yhat - y) # gradient w.r.t. yhat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Create an initialization function to set up model as a dict\n",
    "\n",
    "Write a function `initialize_model` that accepts a list  `dimensions` of positive integer inputs that constructs a `dict` with specific key-value pairs:\n",
    "+ `model['nlayers']` : number of layers in neural network\n",
    "+ `model['weights']` : list of NumPy matrices with appropriate dimensions\n",
    "+ `model['biases']` : list of NumPy (column) vectors of appropriate dimensions\n",
    "+ `model['act_funs']` : list of (vectorised) Python functions (all `sigma` for now)\n",
    "+ `model['act_fun_grads']`: list of (vectorised) Python functions (all `sigma_prime` for now)\n",
    "+ The matrices in `model['weights']` and the vectors in `model['biases']` should be initialized as randomly arrays of the appropriate shapes.\n",
    "\n",
    "If the input list `dimensions` has `L+1` entries, the number of layers is `L` (the first entry of `dimensions` is the input dimension, the next ones are the number of units/neurons in each subsequent layer going up to the output layer).\n",
    "Thus, for example:\n",
    "\n",
    "```python\n",
    ">>> dimensions = [784, 15, 10]\n",
    ">>> model = initialize_model(dimensions)\n",
    ">>> for k, (W, b) in enumerate(zip(model['weights'], model['biases'])):\n",
    ">>>    print(f'Layer {k+1}:\\tShape of W{k+1}: {W.shape}\\tShape of b{k+1}: {b.shape}')\n",
    "```\n",
    "```\n",
    "Layer 1:\tShape of W1: (15, 784)\tShape of b1: (15, 1)\n",
    "Layer 2:\tShape of W2: (10, 15)\tShape of b2: (10, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_model(dimensions, act_funs=[sigma], act_fun_grads=[sigma_prime]):\n",
    "    '''Accepts a list of positive integers; returns a dict 'model' with key/values as follows:\n",
    "      model['nlayers']  : number of layers in neural network\n",
    "      model['weights']  : list of NumPy matrices with appropriate dimensions\n",
    "      model['biases']   : list of NumPy (column) vectors of appropriate dimensions\n",
    "    These correspond to the weight matrices & bias vectors associated with each layer of a neural network.'''\n",
    "    # your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Solution:**\n",
    "```python\n",
    "def initialize_model(dimensions, act_funs=[sigma], act_fun_grads=[sigma_prime]):\n",
    "    '''Accepts a list of positive integers; returns a dict 'model' with key/values as follows:\n",
    "      model['nlayers']  : number of layers in neural network\n",
    "      model['weights']  : list of NumPy matrices with appropriate dimensions\n",
    "      model['biases']   : list of NumPy (column) vectors of appropriate dimensions\n",
    "    These correspond to the weight matrices & bias vectors associated with each layer of a neural network.'''\n",
    "    weights, biases = [], []\n",
    "    L = len(dimensions) - 1 # number of layers (i.e., excludes input layer)\n",
    "    for l in range(L):\n",
    "        W = np.random.randn(dimensions[l+1], dimensions[l])\n",
    "        b = np.random.randn(dimensions[l+1], 1)\n",
    "        weights.append(W)\n",
    "        biases.append(b)\n",
    "    act_funs, grads = [sigma for ell in range(L)], [sigma_prime for ell in range(L)]\n",
    "    return dict(weights=weights, biases=biases, nlayers=L, act_funs=act_funs, act_fun_grads=grads)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a test example to illustrate that the network is initialized as needed\n",
    "dimensions = [784, 15, 10]\n",
    "model = initialize_model(dimensions)\n",
    "for k, (W, b) in enumerate(zip(model['weights'], model['biases'])):\n",
    "    print(f'Layer {k+1}:\\tShape of W{k+1}: {W.shape}\\tShape of b{k+1}: {b.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's examine the weight matrix & bias vector associated with the second layer.\n",
    "print(f'W2:\\n\\n{model[\"weights\"][1]}')  # Expect a 10x15 matrix of random numbers\n",
    "print(f'b2:\\n\\n{model[\"biases\"][1]}')   # Expect a 10x1 vector of random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model['act_funs'])         # Function objects all point to same reference\n",
    "print(model['act_fun_grads'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement a function for forward propagation\n",
    "\n",
    "Write a function `forward` that uses the architecture described in a `dict` as created by `initialize_model` to evaluate the output of the neural network for a given input *column* vector `x`.\n",
    "+ Take $a^{0}=x$ from the input. **Assume the input has features along rows** (the opposite of what we've seen so far).\n",
    "+ For $\\ell=1,\\dotsc,L$, compute & store the intermediate computed vectors $z^{\\ell}=W^{\\ell}a^{\\ell-1}+b^{\\ell}$ (the *weighted inputs*) and $a^{\\ell}=g^{\\ell}\\left(z^{\\ell}\\right)$ (the *activations*) in an updated dictionary `model`. That is, modify the input dictionary `model` so as to accumulate:\n",
    "  + `model['activations']`: a list with entries $a^{\\ell}$ for $\\ell=0,\\dotsc,L$\n",
    "  + `model['weighted_inputs']`: a list with entries $z^{\\ell}$ for $\\ell=1,\\dotsc,L$\n",
    "+ The function should return the computed output $a^{L}$ and the modified dictionary `model`.\n",
    "Notice that input `x` can be a matrix of dimension $n_{0} \\times N_{\\mathrm{batch}}$ corresponding to a batch of input vectors (here, $n_0$ is the dimension of the expected input vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Abstract process into function and run tests again.\n",
    "def forward(x, model):\n",
    "    '''Implementation of forward propagation through a feed-forward neural network.\n",
    "       x : input array oriented column-wise (i.e., features along the rows)\n",
    "       model : dict with same keys as output of initialize_model & appropriate lists in 'weights' & 'biases'\n",
    "    The output dict model is the same as the input with additional keys 'z_inputs' & 'activations';\n",
    "    these are accumulated to be used later for backpropagation. Notice the lists model['z_inputs'] &\n",
    "    model['activations'] both have the same number of entries as model['weights'] & model['biases']\n",
    "    (one for each layer).\n",
    "    '''\n",
    "    # your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Solution:**\n",
    "```python\n",
    "def forward(x, model):\n",
    "    '''Implementation of forward propagation through a feed-forward neural network.\n",
    "       x : input array oriented column-wise (i.e., features along the rows)\n",
    "       model : dict with same keys as output of initialize_model & appropriate lists in 'weights' & 'biases'\n",
    "    The output dict model is the same as the input with additional keys 'z_inputs' & 'activations';\n",
    "    these are accumulated to be used later for backpropagation. Notice the lists model['z_inputs'] &\n",
    "    model['activations'] both have the same number of entries as model['weights'] & model['biases']\n",
    "    (one for each layer).\n",
    "    '''\n",
    "    a = x\n",
    "    activations = [a]\n",
    "    weighted_inputs = []\n",
    "    for W, b, g in zip(model['weights'], model['biases'], model['act_funs']):\n",
    "        z = W @ a + b\n",
    "        a = g(z)\n",
    "        weighted_inputs.append(z)\n",
    "        activations.append(a)\n",
    "    model['activations'], model['weighted_inputs'] = activations, weighted_inputs\n",
    "    return (a, model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a test example to illustrate that the network is initialized as needed\n",
    "dimensions = [784, 15, 10]\n",
    "model = initialize_model(dimensions)\n",
    "print(f'Before executing *forward*:\\nkeys == {model.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_batch = 3  # Let's use, say, 3 random inputs & their corresponding outputs\n",
    "x_input = np.random.rand(dimensions[0], N_batch)\n",
    "y = np.random.rand(dimensions[-1], N_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_hat, model = forward(x_input, model)  # the dict model is *updated* by forward propagation\n",
    "print(f'After executing *forward*:\\nkeys == {model.keys()}')\n",
    "# Observe additional dict keys: 'activations' & 'z_inputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Algorithm for backpropagation:\n",
    "\n",
    "#### (optional reading for the mathematically brave)\n",
    "\n",
    "The description here is based on the *wonderfully concise* description from Michael Neilsen's [*Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com/chap2.html). Neilsen has artfully crafted a summary using the bare minimum mathematical prerequisites. The notation elegantly summarises the important ideas in a way to make implementation easy in array-based frameworks like Matlab or NumPy. This is the best description I know of that does this.\n",
    "\n",
    "In the following, $\\mathcal{L}$ is the loss function and the symbol $\\odot$ is the [*Hadamard product*](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) of two conforming arrays; this is simply a fancy way of writing the usual element-wise product of arrays as computed by NumPy & is sometimes called the *Schur product*. This can be reformulated in usual matrix algebra for analysis.\n",
    "\n",
    "Given a neural network with $L$ layers (not including the \"input layer\") described by an appropriate architecture:\n",
    "\n",
    "1. Input $x$: Set the corresponding activation $a^{0} \\leftarrow x$ for the input layer.\n",
    "2. Feedforward: For each $\\ell=1,2,\\dotsc,L$, compute *weighted inputs* $z^{\\ell}$ & *activations* $a^{\\ell}$ using the formulas\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z^{\\ell} & \\leftarrow  W^{\\ell} a^{\\ell-1} + b^{\\ell}, \\\\\n",
    "a^{\\ell} & \\leftarrow  g_{\\ell} \\left( z^{\\ell}\\right)\n",
    "\\end{aligned}.\n",
    "$$\n",
    "3. Starting from the end, compute the \"error\" in the output layer $\\delta^{L}$ according to the formula\n",
    "$$\n",
    "\\delta^{L} \\leftarrow \\nabla_{a^{L}} \\mathcal{L} \\odot g_{L}'\\left(z^{L}\\right)\n",
    "$$\n",
    "\n",
    "4. *Backpropagate* the \"error\" for $\\ell=L−1\\dotsc,1$ using the formula\n",
    "$$\n",
    "\\delta^{\\ell} \\leftarrow \\left[ W^{\\ell+1}\\right]^{T}\\delta^{\\ell+1} \\odot g_{\\ell}'\\left(z^{\\ell}\\right).\n",
    "$$\n",
    "5. The required gradients of the loss function $\\mathcal{L}$ with respect to the parameters $W^{\\ell}_{p,q}$ and $b^{\\ell}_{r}$ can be computed directly from the \"errors\" $\\left\\{ \\delta^{\\ell} \\right\\}$ and the weighted inputs $\\left\\{ z^{\\ell} \\right\\}$ according to the relations\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial W^{\\ell}_{p,q}} &= a^{\\ell-1}_{q} \\delta^{\\ell}_{p} &&(\\ell=1,\\dotsc,L)\\\\\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial b^{\\ell}_{r}} &= \\delta^{\\ell}_{r} &&\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement a function for backward propagation\n",
    "\n",
    "Implement a function `backward` that implements the back-propagation algorithm to compute the gradients of the loss function $\\mathcal{E}$ with respect to the weight matrices $W^{\\ell}$ and the bias vectors $b^{\\ell}$.\n",
    "+ The function should accept a column vector `y` of output labels and an appropriate dictionary `model` as input.\n",
    "+ The dict `model` is assumed to have been generated *after* a call to `forward`; that is, `model` should have keys `'w_inputs'` and `'activations'` as computed by a call to `forward`.\n",
    "+ The result will be a modified dictionary `model` with two additional key-value pairs:\n",
    "  + `model['grad_weights']`: a list with entries $\\nabla_{W^{\\ell}} \\mathcal{L}$ for $\\ell=1,\\dotsc,L$\n",
    "  + `model['grad_biases']`: a list with entries $\\nabla_{b^{\\ell}} \\mathcal{L}$ for $\\ell=1,\\dotsc,L$\n",
    "+ Notice the dimensions of the matrices $\\nabla_{W^{\\ell}} \\mathcal{L}$ and the vectors $\\nabla_{b^{\\ell}} \\mathcal{L}$ will be identical to those of ${W^{\\ell}}$ and ${b^{\\ell}}$ respectively.\n",
    "+ The function's return value is the modified dictionary `model`.\n",
    "\n",
    "\n",
    "Notice that input `y` can be a matrix of dimension $n_{L} \\times N_{\\mathrm{batch}}$ corresponding to a batch of output vectors (here, $n_L$ is the number of units in the output layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backward(y, model):\n",
    "    '''Implementation of backward propagation of data through the network\n",
    "       y : output array oriented column-wise (i.e., features along the rows) as output by forward\n",
    "       model : dict with same keys as output by forward\n",
    "    Note the input needs to have keys 'nlayers', 'weights', 'biases', 'z_inputs', and 'activations'\n",
    "    '''\n",
    "    # your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Solution:**\n",
    "```python\n",
    "def backward(y, model):\n",
    "    '''Implementation of backward propagation of data through the network\n",
    "       y : output array oriented column-wise (i.e., features along the rows) as output by forward\n",
    "       model : dict with same keys as output by forward\n",
    "    Note the input needs to have keys 'nlayers', 'weights', 'biases', 'z_inputs', and 'activations'\n",
    "    '''\n",
    "    Nbatch = y.shape[1] # Needed to extend for batches of vectors\n",
    "    # Compute the \"error\" delta^L for the output layer\n",
    "    yhat = model['activations'][-1]\n",
    "    z, a = model['weighted_inputs'][-1], model['activations'][-2]\n",
    "    delta = loss_prime(yhat, y) * model['act_fun_grads'][-1](z)\n",
    "    # Use delta^L to compute gradients w.r.t b & W in the output layer.\n",
    "    grad_b, grad_W = delta @ np.ones((Nbatch, 1)), np.dot(delta, a.T)\n",
    "    grad_weights, grad_biases = [grad_W], [grad_b]\n",
    "    loop_iterates = zip(model['weights'][-1:0:-1],\n",
    "                        model['weighted_inputs'][-2::-1],\n",
    "                        model['activations'][-3::-1],\n",
    "                        model['act_fun_grads'][-2::-1])\n",
    "    for W, z, a, g_prime in loop_iterates:\n",
    "        delta = np.dot(W.T, delta) * g_prime(z)\n",
    "        grad_b, grad_W = delta @ np.ones((Nbatch, 1)), np.dot(delta, a.T)\n",
    "        grad_weights.append(grad_W)\n",
    "        grad_biases.append(grad_b)\n",
    "    # We built up lists of gradients backwards, so we reverse the lists\n",
    "    model['grad_weights'], model['grad_biases'] = grad_weights[::-1], grad_biases[::-1]\n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the test example from above. Assume model, x_input have been initialized & *forward* has been executed already.\n",
    "print(f'Before executing *backward*:\\nkeys == {model.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = backward(y, model)  # the dict model is updated *again* by backward propagation\n",
    "print(f'After executing *backward*:\\nkeys == {model.keys()}')\n",
    "# Observe additional dict keys: 'grad_weights' & 'grad_biases'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement a function to update the model parameters using computed gradients.\n",
    "\n",
    "Given some positive learning rate $\\eta>0$, we want to change all the weights and biases using their gradients.\n",
    "Write a function `update` to compute a single step of gradient descent assuming that the model gradients have been computed for a given input vector.\n",
    "+ The functions signature should be `update(eta, model)` where `eta` is a positive scalar value and `model` is a dictionary as output from `backward`.\n",
    "+ The result will be an updated model with the values updated for `model['weights']` and `model['biases']`.\n",
    "+ Written using array notations, these updates can be expressed as\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   W^{\\ell} &\\leftarrow W^{\\ell} - \\eta \\nabla_{W^{\\ell}} \\mathcal{L} &&(\\ell=1,\\dotsc,L)\\\\\n",
    "   b^{\\ell} &\\leftarrow b^{\\ell} - \\eta \\nabla_{b^{\\ell}} \\mathcal{L} &&\n",
    "   \\end{aligned}.\n",
    "   $$\n",
    "+ Written out component-wise, the preceding array expressions would be written as\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "      W^{\\ell}_{p,q} &\\leftarrow W^{\\ell}_{p,q} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W^{\\ell}_{p,q}}\n",
    "      &&(\\ell=1,\\dotsc,L)\\\\\n",
    "      b^{\\ell}_{r} &\\leftarrow b^{\\ell}_{r} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b^{\\ell}_{r}} &&\n",
    "   \\end{aligned}\n",
    "   $$.\n",
    "+ For safety, have the update step delete the keys added by calls to `forward` and `backward`, i.e., the keys `'z_inputs'`, `'activations'`, `'grad_weights'`, & `'grad_biases'`.\n",
    "+ The output should be a dict `model` like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update(eta, model):\n",
    "    '''Use learning rate and gradients to update model parameters\n",
    "       eta : learning rate (positive scalar parameter)\n",
    "       model : dict with same keys as output by backward\n",
    "    Output result is a modified dict model\n",
    "    '''\n",
    "    # your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Solution:**\n",
    "```python\n",
    "def update(eta, model):\n",
    "    '''Use learning rate and gradients to update model parameters\n",
    "       eta : learning rate (positive scalar parameter)\n",
    "       model : dict with same keys as output by backward\n",
    "    Output result is a modified dict model\n",
    "    '''\n",
    "    new_weights, new_biases = [], []\n",
    "    for W, b, dW, db in zip(model['weights'], model['biases'], model['grad_weights'], model['grad_biases']):\n",
    "        new_weights.append(W - (eta * dW))\n",
    "        new_biases.append(b- (eta * db))\n",
    "    model['weights'] = new_weights\n",
    "    model['biases'] = new_biases\n",
    "    # Get rid of extraneous keys/values\n",
    "    for key in ['weighted_inputs', 'activations', 'grad_weights', 'grad_biases']:\n",
    "        del model[key]\n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the test example from above. Assume *forward* & *backward* have been executed already.\n",
    "print(f'Before executing *update*:\\nkeys == {model.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "eta = 0.5  # Choice of learning rate\n",
    "model = update(eta, model)  # the dict model is updated *again* by calling *update*\n",
    "print(f'After executing *update*:\\nkeys == {model.keys()}')\n",
    "# Observe fewer dict keys: extraneous keys have been freed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Observe the required sequence of executions: (forward -> backward -> update -> forward -> backward -> ...)\n",
    "# If done out of sequence, results in KeyError\n",
    "backward(y, model)  # This should cause an exception (KeyError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement steepest descent in a loop for random training data\n",
    "\n",
    "Let's now attempt to use our NumPy-based model to implement the steepest descent algorithm. We'll explain these numbers shortly in the context of the MNIST digit classification problem.\n",
    "\n",
    "+ Generate random arrays `X` and `y` of dimensions $28^2 \\times N_{\\mathrm{batch}}$ and $10\\times N_{\\mathrm{batch}}$ respectively where $N_{\\mathrm{batch}}=10$.\n",
    "+ Initialize the network architecture using `initialize_model` as above to require an input layer of $28^2$ units, a hidden layer of 15 units, and an output layer of 10 units.\n",
    "+ Choose a learning rate of, say, $\\eta=0.1$ and a number of epochs `n_epoch` of, say, $30$.\n",
    "+ Construct a for loop with `n_epochs` iterations in which:\n",
    "    + The output `yhat` is computed from the input`X` using `forward`.\n",
    "    + The function `backward` is called to compute the gradients of the loss function with respect to the weights and biases.\n",
    "    + Update the network parameters using the function `update`.\n",
    "    + Compute and print out the epoch (iteration counter) and the value of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_batch = 10\n",
    "n_epochs = 30\n",
    "dimensions = [784, 15, 10]\n",
    "X = np.random.rand(dimensions[0], N_batch)\n",
    "y = np.random.rand(dimensions[-1], N_batch)\n",
    "eta = 0.1\n",
    "model = ...\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    pass\n",
    "    # your code here ...\n",
    " # Expect to see loss values decreasing systematically in each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Solution:**\n",
    "```python\n",
    "N_batch = 10\n",
    "n_epochs = 30\n",
    "dimensions = [784, 15, 10]\n",
    "X = np.random.rand(dimensions[0], N_batch)\n",
    "y = np.random.rand(dimensions[-1], N_batch)\n",
    "eta = 0.1\n",
    "model = initialize_model(dimensions)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    yhat, model = forward(X, model)\n",
    "    err = loss(yhat, y)\n",
    "    print(f'Epoch: {epoch}\\tLoss: {err}')\n",
    "    model = backward(y, model)\n",
    "    model = update(eta, model)\n",
    "\n",
    " # Expect to see loss values decreasing systematically in each iteration.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Modify the steepest descent loop to make a plot\n",
    "\n",
    "Let's alter the preceding loop to accumulate selected epoch & loss values in lists for plotting.\n",
    "\n",
    "+ Set `N_batch` and `n_epochs` to be larger, say, $500$ and $15,000$ respectively.\n",
    "+ Use a smaller learning rate, say $\\eta=0.00005$\n",
    "+ Change the preceding `for` loop so that:\n",
    "    + The `epoch` counter and the loss value are accumulated into lists every, say, `SKIP` iterations where `SKIP==500`.\n",
    "    + Eliminate the `print` statement(s) to save on output.\n",
    "+ After the `for` loop terminates, make a `semilogy` plot to verify that the loss function is actually decreasing with sucessive epochs.\n",
    "    + Use the list `epochs` to accumulate the `epoch` every 500 epochs.\n",
    "    + Use the list `losses` to accumulate the values of the loss function every 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_batch = 500\n",
    "n_epochs = 15000\n",
    "SKIP = 500\n",
    "dimensions = [784, 15, 10]\n",
    "N_samples = 60000\n",
    "X = np.random.rand(dimensions[0], N_samples)\n",
    "y = np.random.rand(dimensions[-1], N_samples)\n",
    "eta = 0.00001\n",
    "model = initialize_model(dimensions)\n",
    "g = np.random.default_rng()\n",
    "samples = np.arange(N_samples)\n",
    "\n",
    "# accumulate the epochs, losses, & gradient norms in these respective lists\n",
    "epochs, losses, norms = [], [], []\n",
    "for epoch in range(n_epochs):\n",
    "    pass\n",
    "    # your code here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Solution:**\n",
    "```python\n",
    "N_batch = 500\n",
    "n_epochs = 15000\n",
    "SKIP = 500\n",
    "dimensions = [784, 15, 10]\n",
    "N_samples = 60000\n",
    "X = np.random.rand(dimensions[0], N_samples)\n",
    "y = np.random.rand(dimensions[-1], N_samples)\n",
    "eta = 0.00001\n",
    "model = initialize_model(dimensions)\n",
    "g = np.random.default_rng()\n",
    "samples = np.arange(N_samples)\n",
    "\n",
    " # accumulate the epochs, losses, & gradient norms in these respective lists\n",
    "epochs, losses, norms = [], [], []\n",
    "for epoch in range(n_epochs):\n",
    "    batch = sorted(g.choice(samples, size=N_batch, replace=False))\n",
    "    yhat, model = forward(X[:,batch], model)\n",
    "    model = backward(y[:,batch], model)\n",
    "    if (divmod(epoch, SKIP)[1]==0):\n",
    "        err = loss(yhat, y[:,batch])\n",
    "        grad_norm = sum([np.linalg.norm(a.ravel())**2 for a in model['grad_weights']])\n",
    "        grad_norm += sum([np.linalg.norm(a.ravel())**2 for a in model['grad_biases']])\n",
    "        epochs.append(epoch)\n",
    "        losses.append(err)\n",
    "        norms.append(np.sqrt(grad_norm))\n",
    "    model = update(eta, model)\n",
    "\n",
    " # code for plotting once that the lists epochs and losses are accumulated\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlim([0,n_epochs]); ax1.set_ylim([min(losses), max(losses)]);\n",
    "ax1.set_xticks(epochs[::500]); ax1.set_xlabel(\"Epochs\");\n",
    "ax1.set_ylabel(r'$\\mathcal{L}$');\n",
    "h1 = ax1.plot(epochs, losses, 'r-', label=r'$\\mathcal{L}$')\n",
    "ax1.grid(True)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.semilogy(epochs, norms, 'b', label='gradient')\n",
    "ax2.set_ylabel('gradient')\n",
    "ax2.grid(True)\n",
    "fig.legend(loc=(0.65,0.75));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is in fact [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) for this particular loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Using our neural network on MNIST data\n",
    "\n",
    "+  [MNIST database](https://en.wikipedia.org/wiki/MNIST_database) widely used as test problem in machine learning\n",
    " + hand-written decimal numerals (\"digits\")\n",
    " + $60,000$ training images, $10,000$ test images\n",
    " + normalized as $28\\times28$ grayscale images\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"./images/mnist.png\" width=\"50%\"></img></td>\n",
    "    </tr>\n",
    "</center>\n",
    "\n",
    "+ Stored compressed data locally in `.npz` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_PATH = pathlib.Path.cwd() / 'data' / 'mnist.npz'\n",
    "d_file = np.load(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in d_file.keys():\n",
    "    arr = d_file[k]\n",
    "    print(f'Array: {k}\\tShape: {arr.shape} \\tMin: {arr.min():3d}\\tMax: {arr.max():3d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_orig = d_file['X_train']\n",
    "X_test_orig = d_file['X_test']\n",
    "y_train_orig = d_file['y_train']\n",
    "y_test_orig = d_file['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract a single image\n",
    "idx = 45621\n",
    "digit_image = X_train_orig[idx]\n",
    "plt.imshow(digit_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Associated label: {}'.format(y_train_orig[idx]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "#### Exercise: Preprocessing the Digit Features\n",
    "\n",
    "As a first step, preprocess the features in the arrays `X_train_orig` & `X_test_orig`.\n",
    "\n",
    "+ Reshape the three-dimensional arrays into two-dimensional arrays.\n",
    " + Numpy arrays have a method for reshaping (or use the function `np.reshape`).\n",
    "+ Rescale the integer values to be real values between 0 and 1.\n",
    " + Divide the arrays by 255.0 (the grayscale images have integer values between 0 & 255 by default).\n",
    "+ Bind the rescaled & reshaped training & testing arrays to `X_train` & `X_test` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Solution:**\n",
    "```python\n",
    "N_train, nx, ny = X_train_orig.shape\n",
    "X_train = X_train_orig.reshape((N_train, nx*ny))/255.0\n",
    "N_test = len(X_test_orig)\n",
    "X_test = X_test_orig.reshape((N_test, nx*ny))/255.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### For verification:\n",
    "print('X_train: {}\\t{}\\t{}'.format(X_train.shape, X_train.min(), X_train.max()))\n",
    "print('X_test:  {}\\t{}\\t{}'.format(X_test.shape, X_test.min(), X_test.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "#### Exercise: Preprocessing the Targets\n",
    "\n",
    "As a second step, preprocess the targets `y_train_orig` & `y_test_orig` by converting them to two-dimensional arrays with one-hot encoded rows (corresponding to each categorical label).\n",
    "+ The function [`OneHotEncoder` class from `sklearn.preprocessing`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) will do this for you.\n",
    "+ Bind the results to `y_train` and `y_test`.\n",
    "+ Warning: some reshaping to 2D arrays is required for compatibility.\n",
    "+ Warning: the `OneHotEncoder` class produces sparse matrices by default; use the `sparse` keyword to provide a dense array instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert the target arrays y_train_orig & y_test_orig\n",
    "### Assign the results to y_train & y_test respectively.\n",
    "# your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "**Solution:**\n",
    "```python\n",
    " ### Convert the target arrays y_train_orig & y_test_orig\n",
    " ### Assign the results to y_train & y_test respectively.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False).fit(y_train_orig[:,np.newaxis])\n",
    "y_train = encoder.transform(y_train_orig[:,np.newaxis])\n",
    "y_test = encoder.transform(y_test_orig[:,np.newaxis])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### For verification:\n",
    "print('y_train: {}'.format(y_train.shape))\n",
    "print('y_test:  {}'.format(y_test.shape))\n",
    "print(y_test[:3]) # First three rows\n",
    "print(y_test_orig[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Now, assuming that the functions \n",
    "+ `initialize_model`;\n",
    "+ `forward`;\n",
    "+ `backward`; and\n",
    "+ `update`\n",
    "\n",
    "have been defined above, the steepest descent loop can be executed here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "N_batch = 500\n",
    "n_epochs = 15000\n",
    "SKIP = 500\n",
    "dimensions = [784, 15, 10]\n",
    "N_samples = 60000\n",
    "X = X_train.T  # Transpose required to conform in this case\n",
    "y = y_train.T  # Transpose required to conform in this case\n",
    "eta = 0.00005\n",
    "model = initialize_model(dimensions)\n",
    "g = np.random.default_rng()\n",
    "samples = np.arange(N_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# accumulate the epochs, losses, & gradient norms in these respective lists\n",
    "epochs, losses, norms = [], [], []\n",
    "for epoch in range(n_epochs):\n",
    "    batch = sorted(g.choice(samples, size=N_batch, replace=False))\n",
    "    yhat, model = forward(X[:,batch], model)\n",
    "    model = backward(y[:,batch], model)\n",
    "    if (divmod(epoch, SKIP)[1]==0):\n",
    "        err = loss(yhat, y[:,batch])\n",
    "        grad_norm = sum([np.linalg.norm(a.ravel())**2 for a in model['grad_weights']])\n",
    "        grad_norm += sum([np.linalg.norm(a.ravel())**2 for a in model['grad_biases']])\n",
    "        epochs.append(epoch)\n",
    "        losses.append(err)\n",
    "        norms.append(np.sqrt(grad_norm))\n",
    "    model = update(eta, model)\n",
    "\n",
    "# code for plotting once that the lists epochs and losses are accumulated\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlim([0,n_epochs]); ax1.set_ylim([min(losses), max(losses)]);\n",
    "ax1.set_xticks(epochs[::500]); ax1.set_xlabel(\"Epochs\");\n",
    "ax1.set_ylabel(r'$\\mathcal{L}$');\n",
    "h1 = ax1.plot(epochs, losses, 'r-', label=r'$\\mathcal{L}$')\n",
    "ax1.grid(True)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.semilogy(epochs, norms, 'b', label='gradient')\n",
    "ax2.set_ylabel('gradient')\n",
    "ax2.grid(True)\n",
    "fig.legend(loc=(0.65,0.75));    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Essential components of neural networks: activation functions, layers, forward & backward propagation\n",
    "- Building a neural network in NumPy\n",
    "  - Hand-coding stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
