{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Logistic Regression with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Outline\n",
    "\n",
    "1. Using PyTorch data utilities\n",
    "2. Constructing Logistic Regression model in PyTorch\n",
    "3. Training the neural network\n",
    "\n",
    "+ Objective: use logistic regression to solve binary classification problem to examplify workflow with PyTorch\n",
    "  + Framework extends to training much larger deep network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pprint as pp\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Generate simple data: linear binary classification problem in 2D\n",
    "+ Convenient `make_blobs` utility in Scikit-Learn useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 120\n",
    "x, y = datasets.make_blobs(n_samples=n_samples, n_features=2, centers=[(-1,-1),(1.4,1)], cluster_std=[.2,.4], random_state=0)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(x[:,0], x[:,1], c=y, cmap=cm.bwr)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `make_moons` generates more sophisticated geometry\n",
    "+ Linear binary classifier should fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 120\n",
    "x, y = datasets.make_moons(n_samples=n_samples, random_state=0, noise=0.1)\n",
    "x = x - np.mean(x,0) # 0 centered\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(x[:,0], x[:,1], c=y, cmap=cm.bwr)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3)\n",
    "plt.title(f'#data = {n_samples}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using PyTorch data utilities\n",
    "\n",
    "+ Deep learning models data intensive\n",
    "   + Organizing data to support training deep neural networks time-consuming\n",
    "\n",
    "#### PyTorch `Dataset` & `DataLoader` classes\n",
    "\n",
    "+ [PyTorch `Dataset` class](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) (in `torch.utils.data`) for constructing appropriate _data loaders_ for deep network training\n",
    "+ Abstraction generalizes to work with large on-disk data sets\n",
    "+ [PyTorch `DataLoader` class](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) (again, in `torch.utils.data`) combines dataset and sampling strategy (e.g., in random batches without replacement) as Python iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            'feature': torch.tensor(self.x[idx], dtype=torch.float32), \n",
    "            'label': torch.tensor(np.array([self.y[idx]]), dtype=torch.float32)}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "\n",
    "dataset = MyDataset(x, y)\n",
    "print('length: ', len(dataset))\n",
    "for i in range(5):\n",
    "    pp.pprint(dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Construct a `DataLoader` for batches in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = MyDataset(x, y) # Instantiate dataset\n",
    "batch_size = 4\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, samples in enumerate(dataloader):\n",
    "    if i_batch % 10 == 0:\n",
    "        print('\\nbatch# = %s' % i_batch)\n",
    "        print('samples: ')\n",
    "        pp.pprint(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Constructing Logisitic Regression model in PyTorch\n",
    "\n",
    "+ With data ready, next construct PyTorch model\n",
    "+ Inherit model class from PyTorch `nn.Module` class\n",
    "   + Needs to provide `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        output_dim = 1\n",
    "        self.weight = Parameter(torch.Tensor(input_dim, 1))\n",
    "        self.bias = Parameter(torch.Tensor(1, 1))\n",
    "        \n",
    "        stdv = 1.\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a = torch.mm(x, self.weight)\n",
    "        b = torch.mul(torch.ones(x.size(0),1), self.bias)\n",
    "        c = torch.add(a, b)\n",
    "        d = torch.sigmoid(c)#torch.exp(-c)\n",
    "        return d#1./(1. + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Defining a Loss Function\n",
    "\n",
    "+ Can be defined using class `nn.Module` again\n",
    "+ When instantiated, method `MyLoss.forward(predictions, target)` computes *binary cross-entropy*\n",
    "  $$ \\mathcal{L}(\\hat{y}, y) = -\\sum_{k=1}^{N} \\left[ y_{k} \\log\\left( \\hat{y}_{k} \\right) + \\left(1-y_{k}\\right) \\log\\left(1- \\hat{y}_{k} \\right) \\right]$$\n",
    "+ Typical loss function for *classification* problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        log_probs = torch.where(targets.byte(), torch.log(predictions), torch.log(1.-predictions))\n",
    "        loss = - torch.sum(log_probs)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing accuracy\n",
    "\n",
    "+ Useful for classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    p = (predictions > 0.5)\n",
    "    s = torch.sum(p.eq(targets.bool()))\n",
    "    return s.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training the neural network\n",
    "\n",
    "+ Module [`torch.optim`](https://pytorch.org/docs/stable/optim.html) supports numerous oprimization algorithms\n",
    "  + Custom strategies can be defined (e.g., [*stochastic gradient descent*](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), [ADAM](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam), [AdaGrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad), etc.)\n",
    "+ Here, define `optimizer` using `torch.optim.SGD`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model = LogisticRegression(2)\n",
    "criterion = MyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)  \n",
    "\n",
    "dataset = MyDataset(x, y)\n",
    "batch_size = 16\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "training_sample_generator = DataLoader(dataset, \n",
    "                                       batch_size=batch_size, \n",
    "                                       shuffle=shuffle, \n",
    "                                       num_workers=num_workers)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    n = 0\n",
    "    for batch_i, samples in enumerate(training_sample_generator):\n",
    "        predictions = model(samples['feature'])\n",
    "        error = criterion(predictions, samples['label'])\n",
    "        n += accuracy(predictions, samples['label'])\n",
    "        optimizer.zero_grad()\n",
    "        error.backward()        \n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch={epoch:03}. error={error.item():<7.4}. accuracy={n}')\n",
    "    \n",
    "    # If we have achieved 99% accuracy, then stop.\n",
    "    if n > .99 * n_samples: \n",
    "        break\n",
    "print(f'epoch={epoch:03}. error={error.item():<7.4}. accuracy={n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Colors represent whether or not points are classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.zeros(n_samples)\n",
    "prob_of_one = model(torch.Tensor(x)).detach().numpy().flatten()\n",
    "predicted_labels[prob_of_one > 0.5] = 1\n",
    "\n",
    "# Color 1 represent correct classification, 0 otherwise\n",
    "colors = np.where(predicted_labels == y, 1, 0) \n",
    "\n",
    "acc = np.sum(colors)\n",
    "print(f'accuracy = {acc/len(colors)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.title('Classification results')\n",
    "plt.scatter(x[:,0], x[:,1], c=colors, cmap=cm.bwr)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcoord = np.linspace(-3, 3)\n",
    "ycoord = np.linspace(-3, 3)\n",
    "xx, yy = np.meshgrid(xcoord, ycoord)\n",
    "xxt = torch.tensor(xx, dtype=torch.float32).view(-1).unsqueeze(0)\n",
    "yyt = torch.tensor(yy, dtype=torch.float32).view(-1).unsqueeze(0)\n",
    "v = torch.t(torch.cat([xxt,yyt]))\n",
    "m = model(v)\n",
    "mm = m.detach().numpy().reshape(50,50)\n",
    "\n",
    "x_try = torch.tensor(x, dtype=torch.float32)\n",
    "y_try = model(x_try)\n",
    "yy_try = (y_try.squeeze() > 0.5).numpy()\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "extent = -3, 3, -3, 3\n",
    "plt.imshow(mm, cmap=cm.BuGn, interpolation='bilinear', extent=extent, alpha=.5, origin='lower')\n",
    "plt.scatter(x[:,0], x[:,1], c=yy_try, cmap=cm.viridis)\n",
    "plt.colorbar()\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3)\n",
    "plt.title('Classification results');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "+ Using PyTorch data utilities\n",
    "+ Constructing Logistic Regression model in PyTorch\n",
    "+ Training the neural network\n",
    "+ Framework extends to training much larger deep network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "filesystem-cybera",
   "language": "python",
   "name": "conda-env-filesystem-cybera-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
