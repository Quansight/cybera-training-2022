{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Mathematics of Deep Neural Networks\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- A layered view of deep networks\n",
    "- Recursive nature of deep networks\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression and layers\n",
    "\n",
    "Lets look at how we can specify logistic regression as layers.  The ability to specify such models as layers is key to designing deep neural networks.  We will also discuss *backpropagation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: two-class softmax classifier\n",
    "\n",
    "Lets consider a binary classification problem.  We often solve this problem using a sigmoid function.  The idea is that given a probability $p$ of one class, we can compute the probability of the other class as $1-p$.  For this discussion, however, we employ softmax and explicity models the two probabilities.  This discussion can easily be extended to more-than-two-classes classification problems.\n",
    "\n",
    "Data consists of $(\\mathbf{x}^{(i)}, y^{(i)})$.  Here $i \\in [1,N]$, $\\mathbf{x}^{(i)} \\in \\mathbb{R}^M$ is the ith feature vector and $y^{(i)}$ is the associated class label.  \n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/two-class-linear.png\" width=35%>\n",
    "        </td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative log likelihood\n",
    "\n",
    "We use negative log likelihood cost to train this classifier\n",
    "\n",
    "$$\n",
    "\\newcommand{bx}{\\mathbf{x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\small\n",
    "\\begin{split}\n",
    "l(\\theta) = - \\sum_{i=1}^N \\mathbb{I}_0 (y^{(i)}) \\log \n",
    "\\frac{e^{\\bx^{(i)^T} \\theta_1}}{e^{\\bx^{(i)^T} \\theta_1} + {e^{\\bx^{(i)^T} \\theta_2}}}\n",
    "+\n",
    "\\mathbb{I}_1 (y^{(i)}) \\log \n",
    "\\frac{e^{\\bx^{(i)^T} \\theta_2}}{e^{\\bx^{(i)^T} \\theta_1} + {e^{\\bx^{(i)^T} \\theta_2}}}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Define cost $C(\\theta)$ that we want to minimize to be the negative log likelihood $l(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer representation\n",
    "\n",
    "Lets represent the above network as a layers.  We will treat cost as a layer as well.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/softmax-2classes-layers.png\" width=55%>\n",
    "        </td>\n",
    "    </tr>\n",
    "</center>\n",
    "\n",
    "In order to train this classifier, we need to estimate values for weights $\\theta_1$ and $\\theta_2$ ($\\theta_1, \\theta_2 \\in \\mathbb{R}^{M+1}$).  In order to do so, we need to compute $\\frac{\\partial C(\\theta)}{\\partial \\theta_1}$ and $\\frac{\\partial C(\\theta)}{\\partial \\theta_2}$.  Note that we set $z^4 = C(\\theta)$, so we are interested in $\\frac{\\partial z^4}{\\partial \\theta_1}$ and $\\frac{\\partial z^4}{\\partial \\theta_2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain rule\n",
    "\n",
    "A formula for computing derivatives of composite functions (Gottfried Wilhelm Leibniz, 1676).\n",
    "\n",
    "$$\n",
    "\\newcommand{\\diff}[2]\n",
    "{\n",
    "\\frac{\\partial {#1}}{\\partial {#2}}\n",
    "}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(g(u,v),h(u,v))}{\\partial u} = \\diff{f}{g} \\diff{g}{u} + \\diff{f}{h} \\diff{h}{u}\n",
    "$$\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/chain-rule.png\" width=25%>\n",
    "        </td>\n",
    "    </tr>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing derivatives of loss with respect to weights (i.e., network parameters)\n",
    "\n",
    "We can use the *chain rule* to compute $\\frac{\\partial z^4}{\\partial \\theta_1}$ and $\\frac{\\partial z^4}{\\partial \\theta_2}$.  \n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/softmax-2classes-layers.png\" width=55%>\n",
    "        </td>\n",
    "    </tr>\n",
    "</center>\n",
    "\n",
    "$$\n",
    "\\newcommand{\\diff}[2]\n",
    "{\n",
    "\\frac{\\partial {#1}}{\\partial {#2}}\n",
    "}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\diff{z^4}{\\theta_1} &= \\diff{z^4}{z_1^3} \\diff{z_1^3}{\\theta_1} + \\diff{z^4}{z_2^3} \\diff{z_2^3}{\\theta_1} \\\\\n",
    "&= \\diff{z^4}{z_1^3} \\left( \\diff{z_1^3}{z_1^2} \\diff{z_1^2}{\\theta_1} +  \\diff{z_1^3}{z_2^2} \\diff{z_2^2}{\\theta_1}  \\right) \\\\ \n",
    "&+ \\diff{z^4}{z_2^3} \\left( \\diff{z_2^3}{z_1^2} \\diff{z_1^2}{\\theta_1} +  \\diff{z_2^3}{z_2^2} \\diff{z_2^2}{\\theta_1}  \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<!-- <center>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/chain-rule-z4.png\" width=35%>\n",
    "        </td>\n",
    "    </tr>\n",
    "</center>\n",
    " -->\n",
    "We can similarly compute $\\frac{\\partial z^4}{\\partial \\theta_2}$.\n",
    "\n",
    "Recall that $z^4 = C(\\theta)$, and we can minimize the $C(\\theta)$ using gradient descent using the gradients computed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/softmax-2classes-layers2.png\" width=45%>\n",
    "        </td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass**\n",
    "\n",
    "$z^1 = f(\\mathbf{x})$ (*input data*)\\\n",
    "$z^2 = f(z^1)$ (*linear function*)\\\n",
    "$z^3 = f(z^2)$ (*log softmax*)\\\n",
    "$z^4 = f(z^3) = C(\\theta)$ (*negative log likelihood*, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "$\\delta^l = \\diff{C(\\theta)}{z^L}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing $\\delta^l$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\delta^4 &= \\diff{C({\\theta})}{z^4} = \\diff{z^4}{z^4} = 1 \\\\\n",
    "\\delta^3_1 &= \\diff{C(\\theta)}{z^3_1} = \\diff{C(\\theta)}{z^4} \\diff{z^4}{z^3_1} \n",
    "= \\delta^4 \\diff{z^4}{z^3_1} \\\\\n",
    "\\delta^3_2 &= \\diff{C(\\theta)}{z^3_2} = \\diff{C(\\theta)}{z^4} \\diff{z^4}{z^3_2} \n",
    "= \\delta^4 \\diff{z^4}{z^3_2} \\\\\n",
    "\\delta^2_1 &= \\diff{C(\\theta)}{z^2_1} = \\sum_k \\diff{C(\\theta)}{z^3_k} \\diff{z^3_k}{z^2_1} = \\sum_k \\delta^3_k \\diff{z^3_k}{z^2_1} \\\\\n",
    "\\delta^2_2 &= \\diff{C(\\theta)}{z^2_2} = \\sum_k \\diff{C(\\theta)}{z^3_k} \\diff{z^3_k}{z^2_2} = \\sum_k \\delta^3_k \\diff{z^3_k}{z^2_2}\n",
    "\\end{split} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For any differentiable layer $l$\n",
    "\n",
    "For a given layer $l$, with inputs $z_i^l$ and outputs $z_k^{l+1}$\n",
    "$$\n",
    "\\delta^l_i = \\sum_k \\delta^{l+1}_k \\diff{z^{l+1}_k}{z^l_i}\n",
    "$$\n",
    "\n",
    "Similarly, for layer $l$ that depends upon parameters $\\theta^l$,\n",
    "$$\n",
    "\\diff{C(\\theta)}{\\theta^l} = \\sum_k \\diff{C(\\theta)}{z^{l+1}_k} \\diff{z^{l+1}_k}{\\theta^l} = \\sum_k \\delta^{l+1}_k \\diff{z^{l+1}_k}{\\theta^l}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For layer 1 in our two-class softmax classifier\n",
    "\n",
    "In our 2-class softmax classifier only layer 1 has parameters ($\\theta_0$ and $\\theta_1$).\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/layer-l.png\" width=25%>\n",
    "        </td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layered architectures\n",
    "\n",
    "As long as we have differentiable layers, i.e., we can compute $\\diff{z^{l+1}_k}{z^{l}_i}$,\n",
    "we can use *backpropagation* to update the parameters $\\theta$ to minimize the cost $C(\\theta)$.\n",
    "\n",
    "<center>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/layer-architecture.png\" width=55%>\n",
    "        </td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "- Set $z^1$ equal to input $\\mathbf{x}$.\n",
    "- Forward pass: compute $z^2, z^3, ...$ layers $1, 2, ...$ activations.\n",
    "- Set $\\delta$ at the last layer equal to 1\n",
    "- Backward pass: backpropagate $\\delta$s all the way to first layer.\n",
    "- Update $\\theta$\n",
    "- Repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Layered view of logistic regression and softmax\n",
    "- Backpropogation\n",
    "- Deep networks are recursive - we can treat a deep network as a layer in another network (this is extremely powerful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
