{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Linear Regression with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Outline\n",
    "\n",
    "1. Using PyTorch data utilities\n",
    "2. Constructing Linear Regression model in PyTorch\n",
    "3. Training the neural network\n",
    "\n",
    "+ Objective: use 1D linear regression as example of workflow with PyTorch\n",
    "  + Framework extends to training much larger deep network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Generate simple data: straight line with some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10 # number of data points\n",
    "m = .7\n",
    "c = 0\n",
    "x = np.linspace(0, 9, n_samples) \n",
    "y = m*x + c + np.random.normal(0,.3,x.shape) - 3\n",
    "plt.figure()\n",
    "plt.plot(x,y,'o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim(-1,10)\n",
    "plt.ylim(-4,4)\n",
    "plt.title(f'2D data (#data = {n_samples})');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ More interesting data; line obviously poor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 10\n",
    "x = np.arange(n_samples)\n",
    "y = np.sin(2 * np.pi * x / n_samples) * 4\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(x, y, 'o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim(-1,10)\n",
    "plt.ylim(-5,5)\n",
    "plt.title(f'2D data (#data = {n_samples})');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using PyTorch data utilities\n",
    "\n",
    "+ Deep learning models are data intensive\n",
    "   + Organizing data to support training deep neural networks time-consuming\n",
    "\n",
    "#### PyTorch `Dataset` & `DataLoader` classes\n",
    "\n",
    "+ [PyTorch `Dataset` class](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) (in `torch.utils.data`) for constructing appropriate _data loaders_ for deep network training\n",
    "+ Abstraction generalizes to work with large on-disk data sets\n",
    "+ [PyTorch `DataLoader` class](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) (again, in `torch.utils.data`) combines dataset and sampling strategy (e.g., in random batches without replacement) as Python iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y       \n",
    "    def __len__(self):\n",
    "        return len(self.x)    \n",
    "    def __getitem__(self, idx):\n",
    "        # Notice 'feature' returns homogeneous tuple corresponding to x\n",
    "        sample = {\n",
    "            'feature': torch.tensor([1,self.x[idx]], dtype=torch.double), \n",
    "            'label': torch.tensor([self.y[idx]], dtype=torch.double)}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy arrays of data created previously\n",
    "print(f'Features shape: {x.shape}')\n",
    "print(f'Targets shape {y.shape}')\n",
    "# Wraps in MyDataSet class around features & labels\n",
    "dataset = MyDataset(x, y)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, sample in enumerate(dataset):\n",
    "    print(f\"Observation{k:2d}:\\tFeatures: {sample['feature']}\\tLabel: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Construct a `DataLoader` for batches in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = MyDataset(x, y) # Instantiate dataset\n",
    "batch_size = 3\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, samples in enumerate(dataloader):\n",
    "    print('\\nbatch# = %s' % i_batch)\n",
    "    print('samples: ')\n",
    "    pp.pprint(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Constructing Linear Regression model in PyTorch\n",
    "\n",
    "+ With data ready, next construct PyTorch model\n",
    "+ Inherit model class from PyTorch `nn.Module` class\n",
    "   + Needs to provide `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, dim_input, dim_output):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        # Defining model parameters m and c\n",
    "        # for model y = mx + c.\n",
    "        # Specififally, m represents weight matrix,\n",
    "        #  c represents bias vector\n",
    "        self.weight = Parameter(torch.empty([dim_output, dim_input], dtype=torch.double))\n",
    "        self.bias = Parameter(torch.empty([dim_output, 1], dtype=torch.double))\n",
    "        \n",
    "        # Initializing parameter values uniformly distributed random values\n",
    "        stdv = 1.0\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        weight_and_bias = torch.cat((self.weight, self.bias), 1)\n",
    "        # Output of model is y = [x, 1].T * [m, c]\n",
    "        # Important: input x may not be single observation. \n",
    "        # Input x may be batch of inputs as tensor\n",
    "        #\n",
    "        # (Common to use dimension 0 use batch dimension)\n",
    "        out = x.matmul(weight_and_bias.t())\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Instantiate model for 1D linear regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "model = MyModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `dataloader` from before draws samples from data set in batches of fixed size\n",
    "+ `model.forward` is prediction from model with current `weight`/`bias` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_batch, sample in enumerate(dataloader):\n",
    "    print(f'Batch number {k_batch}')\n",
    "    prediction = model.forward(sample['feature'])\n",
    "    print('Prediction:\\n', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Defining a Loss Function\n",
    "\n",
    "+ Can be defined using class `nn.Module` again\n",
    "+ When instantiated, method `MyLoss.forward(predictions, target)` computes loss (squared error)\n",
    "  $$ \\mathcal{L}(\\hat{y}, y) = \\sum_{k=1}^{N}  \\left[ y_{k} - \\hat{y}_{k} \\right]^2 $$\n",
    "+ Typical loss function for *regression* problems (not scaled below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, predictions, targets):       \n",
    "        diff = torch.sub(predictions, targets)\n",
    "        diff2 = torch.pow(diff, 2)\n",
    "        err = torch.sum(diff2)\n",
    "        return err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training the neural network\n",
    "\n",
    "+ Module [`torch.optim`](https://pytorch.org/docs/stable/optim.html) supports numerous oprimization algorithms\n",
    "  + Custom strategies can be defined (e.g., [*stochastic gradient descent*](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), [ADAM](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam), [AdaGrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad), etc.)\n",
    "+ Here, define `optimizer` using `torch.optim.SGD`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000  # How many times the entire training data is seen?\n",
    "l_rate = 0.01\n",
    "# Instantiate objects for optimizer & loss function\n",
    "#   Requires model.parameters() & a learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = l_rate)\n",
    "loss = MyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize dataset & dataloader, just in case\n",
    "dataset = MyDataset(x, y)\n",
    "batch_size = 4\n",
    "shuffle = True\n",
    "num_workers = 4\n",
    "training_data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(training_data_loader))\n",
    "print(model(sample['feature']))\n",
    "print(model.forward(sample['feature']))\n",
    "print('Model weight', model.weight)\n",
    "print('Model bias', model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch = %s' % epoch)\n",
    "    for k_batch, samples in enumerate(training_data_loader):\n",
    "        predictions = model.forward(samples['feature'])\n",
    "        error = loss.forward(predictions, samples['label'])\n",
    "        if epoch % 100 == 0:\n",
    "            print('\\tBatch = %s, Error = %s' % (k_batch, error.item()))\n",
    "        \n",
    "        # Before the backward pass, use optimizer object to zero out all\n",
    "        # gradients for variables to update (i.e., learnable parameters).\n",
    "        # By default, gradients are accumulated in buffers (i.e., not overwritten)\n",
    "        # whenever .backward() called.\n",
    "        # See docs of torch.autograd.backward for details\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Backward pass: compute gradient of loss wrt model parameters\n",
    "        error.backward()\n",
    "        \n",
    "        # Calling method optimizer.step to compute updated parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing result of training\n",
    "\n",
    "+ Make grid of values to sample `model`\n",
    "+ Preprocess inputs into form suitable for `model.forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_for_plotting = np.linspace(-1, 10, 1000)\n",
    "design_matrix = torch.tensor(np.vstack([np.ones(x_for_plotting.shape), x_for_plotting]).T, dtype=torch.double)\n",
    "print('Design matrix shape:', design_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_for_plotting = model.forward(design_matrix)\n",
    "print('y_for_plotting shape:', y_for_plotting.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(x, y, 'o', label='data')\n",
    "plt.plot(x_for_plotting, y_for_plotting.data.numpy(), 'r-', label='model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim(-1,10)\n",
    "plt.ylim(-10,10)\n",
    "plt.title('Current fit:')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Data & linear regression model');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model for later use\n",
    "+ `model` has method `state_dict` that returns parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for key, val in model.state_dict().items():\n",
    "    print(f'Key: {key}\\tValue: {val}\\t(Size: {val.size()})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "filepath = pathlib.Path('.') / 'model01.pt'\n",
    "torch.save(model.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading saved model\n",
    "+ `nn.Module` includes `load_state_dict` method for retrieving models from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, x_for_plotting, y_for_plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = MyModel(1, 1)\n",
    "model_loaded.load_state_dict(torch.load('model01.pt'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_for_plotting = np.linspace(-1, 10, 1000)\n",
    "design_matrix = torch.tensor(np.vstack([np.ones(x_for_plotting.shape), x_for_plotting]).T, dtype=torch.double)\n",
    "print('Design matrix shape:', design_matrix.shape)\n",
    "\n",
    "y_for_plotting = model_loaded.forward(design_matrix)\n",
    "print('y_for_plotting shape:', y_for_plotting.shape)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(x, y, 'o', label='data')\n",
    "plt.plot(x_for_plotting, y_for_plotting.data.numpy(), 'r-', label='model')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim(-1,10)\n",
    "plt.ylim(-10,10)\n",
    "plt.title('Current fit:')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Data & linear regression model');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "+ Using PyTorch data utilities\n",
    "+ Constructing Linear Regression model in PyTorch\n",
    "+ Training the neural network\n",
    "+ Framework extends to training much larger deep network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "filesystem-cybera",
   "language": "python",
   "name": "conda-env-filesystem-cybera-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
