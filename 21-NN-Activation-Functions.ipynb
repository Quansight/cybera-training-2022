{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Common Activation Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Model\n",
    "\n",
    "A perceptron model takes an input, computes its weighted sum, and either fires or doesn't.  A single perceptron can implement *linearly separable* functions.  The decision when to fire (or not) is an important one.  We discuss activation functions as mechanism that determine whether or not a neuron fires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step function\n",
    "\n",
    "The simplest model for an activation function.  Neuron fires (or is equal to 1) if the weighted sum is greater than some threshold.  \n",
    "\n",
    "$$\n",
    "y = \n",
    "\\begin{cases}\n",
    "1 & x \\ge \\mathrm{threshold} \\\\\n",
    "0 & \\mathrm{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Here $y$ represents neuron activation, $x$ is the weighted sum input to the neuron, and $\\mathrm{threshold}$ is the neuron-specific threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(np.linspace(-10, 10, 100))\n",
    "threshold = 2.5\n",
    "\n",
    "y = np.where(x >= threshold, 1, 0)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(x, y, 'r');\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Step function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activation function is used in the *Perceptron Learning Algorithm*, originally proposed by Frank Rosenblatt in 1943 and later refined by Minsky and Papert in 1969.\n",
    "\n",
    "Step function can work well for binary classification problems; however, how would we use for a multi-class classifier problem?  What if more than one neuron is activated for a given input example.  It would be difficult for us to determine which neuron is \"more\" activated then other neurons that are also activated?  It would be good to have a mechanism where neuron activation is not binary.  Rather a neuron can be fired anywhere between 0 percent and 100 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear function\n",
    "\n",
    "One choice for non-binary activations is to use linear function.  Neuron activation is proportional to its input (i.e., weighted sum).  \n",
    "\n",
    "$$\n",
    "y = c x\n",
    "$$\n",
    "\n",
    "Here $y$ represents neuron activation, $x$ is the weighted sum input to the neuron, and $c$ is a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(np.linspace(-10, 10, 100))\n",
    "c = 2\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(c*x,'r-',label='output');\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Linear function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this function solves the problem binary activation problem, it suffers from the following issues:\n",
    "\n",
    "- The gradient is constant, and doesn't depend upon the input.\n",
    "- The function is linear, and this means that even if we have multiple layers, the network can only represent linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1 + \\exp(-x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(np.linspace(-10, 10, 100))\n",
    "m = nn.Sigmoid()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(m(x),'r-',label='output');\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Sigmoid');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros\n",
    "\n",
    "- Step-like smooth function\n",
    "- Non-binary activation\n",
    "- Non-linear, so we can stack layers together and can approximate non-linear functions\n",
    "- Values lie between $0$ and $1$\n",
    "- Most widely used activation functions in neural networks\n",
    "\n",
    "### Cons\n",
    "\n",
    "- Suffer from *vanishing gradients* problems; if the input $x$ is too far away from $0$, the gradients become too small.  Training slows down or stops completely.\n",
    "- One way around this problem is to scale the inputs $x$ to avoid ranges where the output becomes flat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h &= \\tanh(x) \\\\\n",
    "&= \\frac{2}{1 + \\exp(-2x)} - 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This function has similar properties to Sigmoid.  The gradients are steeper than Sigmoid around $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(np.linspace(-10, 10, 100))\n",
    "m = nn.Tanh()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(m(x),'r-',label='output');\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Tanh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(np.linspace(-10, 10, 100))\n",
    "m = nn.ReLU()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(x, m(x), 'r');\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('ReLU');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y = \\max(0, x)\n",
    "$$\n",
    "\n",
    "- Most widely used activation function in deep neural networks.\n",
    "- ReLU is *non-linear*, so we can stack layers to approximate non-linear functions.\n",
    "- It has \"sparsity activation\" property, which is very useful.  Sparsity activation refers to the fact then a number of neurons will be turned off (note that for inputs less than $0$, ReLU is off).  Sparse activations make networks more efficient. \n",
    "- Computationally less expensive than Sigmoid or Tanh.\n",
    "\n",
    "### Cons\n",
    "\n",
    "- ReLU do suffer from dying ReLU problems.  This happens when an input to ReLU is less than $0$.  The gradients for values less than $x$ is $0$.  This can cause a neuron to stop learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(np.linspace(-10, 10, 100))\n",
    "m = nn.LeakyReLU(0.1)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(x, m(x), 'r');\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Leaky ReLU');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attempts to address the dying ReLU problem\n",
    "- Doesn't offer \"sparsity activation\" property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5)\n",
    "m = nn.Softmax(dim=0)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(x,'b-.',label='input vector')\n",
    "plt.plot(m(x),'r-',label='output vector');\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Softmax');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used in multi-class classification problems\n",
    "- Turns input vector in to a probability distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softshrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(np.linspace(-10, 10, 100))\n",
    "m = nn.Softshrink(lambd=2.5)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "#plt.plot(x,'b-.',label='input vector')\n",
    "plt.plot(x, m(x),'r-',label='output vector');\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.title('Softshrink');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which activation function to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sigmoid works well for classifier\n",
    "- ReLU works well for internal layers of deep layers\n",
    "- Rule of thumb: pick an activation function that leads to faster training ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <tr>\n",
    "    <td><img src=\"images/Quansight_Logo_Lockup_1.png\" width=\"25%\"></img></td>\n",
    "    </tr>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "filesystem-cybera",
   "language": "python",
   "name": "conda-env-filesystem-cybera-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
